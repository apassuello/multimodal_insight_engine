# Final Implementation Plan: Dual Model + Content Logging

**Date:** 2025-11-17
**Status:** APPROVED - Ready for Implementation
**Target:** Visible improvement in demo with full transparency

---

## Approved Configuration: Option 2 (Dual Model)

### Model Selection

| Role | Model | Parameters | Memory | Rationale |
|------|-------|------------|--------|-----------|
| **Evaluation + Critique** | **Qwen2-1.5B-Instruct** | 1.5B | 3GB | Best instruction-following, least hallucination |
| **Generation (train this)** | **Phi-2** | 2.7B | 5.4GB | Best for fine-tuning, rivals 7B after training |
| **Total** | - | **4.2B** | **8.4GB** | Within approved memory budget |

### Why This Configuration

**Qwen2-1.5B-Instruct for Evaluation:**
- Designed for instruction-following tasks
- Best at TruthfulQA (least hallucination) - critical for evaluation
- Outperforms Phi-2 on language understanding
- Will judge Phi-2's outputs objectively

**Phi-2 for Generation:**
- "After fine-tuning, beats 7B models" (2024 benchmark)
- Best in class for learning from training data
- Different architecture from evaluator (reduces bias)
- 22x larger than current GPT-2 (124M â†’ 2.7B)

**Expected Improvement:**
- Harm detection: 40% â†’ 85-90%
- Training impact: 5% â†’ 40-50% violation reduction
- Text quality: Measurably more nuanced and respectful

---

## Success Criteria

âœ… **Visible improvement in demo:**
- Loaded harmful examples get flagged
- Training shows measurable before/after differences
- Side-by-side comparisons clearly show improvement

âœ… **Transparent logging:**
- Can see actual text at each pipeline stage
- Critiques are meaningful (not generic)
- Revisions actually fix identified issues

âœ… **System stability:**
- All existing tests pass
- No regressions in functionality
- Memory usage within limits

---

## Implementation Phases

### Phase 1: Content Logging Infrastructure (Day 1-2, ~3 hours)

**Goal:** See what's actually happening in the pipeline

#### Step 1.1: Create Content Logger Module
```python
# File: demo/utils/content_logger.py

import logging
import time
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

@dataclass
class ContentLog:
    """Single log entry with content and metadata."""
    stage: str
    content: str
    metadata: Dict[str, Any]
    timestamp: float

class ContentLogger:
    """Logger that shows actual content at each pipeline stage."""

    def __init__(self, verbosity: int = 2):
        """
        Initialize content logger.

        Args:
            verbosity: 0=off, 1=summary, 2=key stages, 3=full pipeline
        """
        self.verbosity = verbosity
        self.logs: List[ContentLog] = []

    def log_stage(
        self,
        stage: str,
        content: str,
        metadata: Optional[Dict[str, Any]] = None,
        truncate: int = 500
    ):
        """Log a pipeline stage with content visibility."""
        if self.verbosity == 0:
            return

        # Store full content
        self.logs.append(ContentLog(
            stage=stage,
            content=content,
            metadata=metadata or {},
            timestamp=time.time()
        ))

        # Display with formatting
        separator = "=" * 60
        print(f"\n[{stage}] {separator}")

        # Truncate for display
        display_content = content if len(content) <= truncate else content[:truncate] + "..."
        print(display_content)

        # Show metadata if verbosity >= 2
        if metadata and self.verbosity >= 2:
            print(f"\nMetadata: {metadata}")

    def log_comparison(
        self,
        label1: str,
        text1: str,
        label2: str,
        text2: str,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """Log side-by-side comparison."""
        if self.verbosity == 0:
            return

        separator = "=" * 60
        print(f"\n[COMPARISON] {separator}")
        print(f"\n{label1}:")
        print(f"  {text1[:250]}...")
        print(f"\n{label2}:")
        print(f"  {text2[:250]}...")

        if metadata:
            print(f"\nComparison Metrics: {metadata}")

        # Highlight key differences
        self._highlight_differences(text1, text2)

    def _highlight_differences(self, text1: str, text2: str):
        """Highlight key textual differences."""
        print(f"\n[DIFF]")

        # Detect common improvements
        if "should" in text1.lower() and "consider" in text2.lower():
            print("  âœ“ Changed prescriptive ('should') to suggestive ('consider')")

        if "must" in text1.lower() and "might" in text2.lower():
            print("  âœ“ Changed mandatory to optional language")

        if "all" in text1.lower() and "some" in text2.lower():
            print("  âœ“ Changed absolute to qualified statement")

    def export_logs(self, filepath: str):
        """Export full logs to JSON for analysis."""
        import json

        with open(filepath, 'w') as f:
            json.dump([
                {
                    "stage": log.stage,
                    "content": log.content,
                    "metadata": log.metadata,
                    "timestamp": log.timestamp
                }
                for log in self.logs
            ], f, indent=2)

        print(f"\nâœ“ Logs exported to {filepath}")

    def get_summary(self) -> str:
        """Get summary of logged activity."""
        if not self.logs:
            return "No activity logged"

        stages = {}
        for log in self.logs:
            stage_type = log.stage.split('-')[0]
            stages[stage_type] = stages.get(stage_type, 0) + 1

        summary = "Activity Summary:\n"
        for stage, count in sorted(stages.items()):
            summary += f"  {stage}: {count} entries\n"

        return summary
```

**Files created:**
- `demo/utils/content_logger.py` (new)
- `demo/utils/__init__.py` (new, empty)

---

#### Step 1.2: Integrate Logging into Evaluation Pipeline

**File:** `src/safety/constitutional/principles.py`

**Add parameter to evaluation functions:**
```python
def _evaluate_harm_with_ai(
    text: str,
    model,
    tokenizer,
    device,
    logger: Optional['ContentLogger'] = None  # NEW
) -> Dict[str, Any]:
    """Evaluate harm with AI (with optional logging)."""

    if logger:
        logger.log_stage("EVAL-INPUT", text)

    prompt = HARM_EVALUATION_PROMPT.format(text=text)

    if logger:
        logger.log_stage("EVAL-PROMPT", prompt, truncate=300)

    config = GenerationConfig(
        max_length=512,
        temperature=0.3,
        do_sample=True
    )

    try:
        response = generate_text(model, tokenizer, prompt, config, device)

        if logger:
            logger.log_stage("EVAL-RAW-OUTPUT", response)

        result = _parse_json_response(response, default_structure)

        if logger:
            logger.log_stage(
                "EVAL-PARSED",
                f"Flagged: {result.get('flagged', False)}",
                metadata=result
            )

        result["method"] = "ai_evaluation"
        return result

    except Exception as e:
        if logger:
            logger.log_stage(
                "EVAL-ERROR",
                f"AI evaluation failed: {e}, falling back to regex"
            )
        return _evaluate_harm_with_regex(text)
```

**Similar updates for:**
- `_evaluate_truthfulness_with_ai()`
- `_evaluate_fairness_with_ai()`
- `_evaluate_autonomy_with_ai()`
- `evaluate_harm_potential()` (pass logger through)

---

#### Step 1.3: Integrate Logging into Training Pipeline

**File:** `demo/managers/training_manager.py`

**Update `train_model()` signature:**
```python
def train_model(
    self,
    model,
    tokenizer,
    framework: ConstitutionalFramework,
    device,
    training_prompts: List[str],
    config: TrainingConfig,
    progress_callback: Optional[Callable[[str, float], None]] = None,
    checkpoint_callback: Optional[Callable[[int, Dict[str, Any]], None]] = None,
    logger: Optional['ContentLogger'] = None  # NEW
) -> Tuple[Dict[str, Any], bool, str]:
```

**Add logging to training data generation:**
```python
# Inside critique_revision_pipeline call area
for idx, prompt in enumerate(prompts):
    if logger:
        logger.log_stage(
            f"TRAINING-EXAMPLE {idx+1}/{len(prompts)}",
            f"Prompt: {prompt}"
        )

    # Generate initial response
    initial = generate_text(model, tokenizer, prompt, ...)

    if logger:
        logger.log_stage("INITIAL-GENERATION", initial)

    # Evaluate initial
    initial_eval = framework.evaluate_text(initial)

    if logger:
        logger.log_stage(
            "INITIAL-EVALUATION",
            f"Violations: {initial_eval['flagged_principles']}\nScore: {initial_eval['weighted_score']}",
            metadata=initial_eval
        )

    # Get critique
    critique = critique_fn(...)

    if logger:
        logger.log_stage("CRITIQUE-OUTPUT", critique)

    # Get revision
    revision = revision_fn(...)

    if logger:
        logger.log_stage("REVISION-OUTPUT", revision)

    # Evaluate revision
    revision_eval = framework.evaluate_text(revision)

    if logger:
        logger.log_stage(
            "REVISION-EVALUATION",
            f"Violations: {revision_eval['flagged_principles']}\nScore: {revision_eval['weighted_score']}",
            metadata=revision_eval
        )

        improvement = initial_eval['weighted_score'] - revision_eval['weighted_score']
        logger.log_stage(
            "TRAINING-PAIR-CREATED",
            f"âœ“ Improvement: {initial_eval['weighted_score']:.1f} â†’ {revision_eval['weighted_score']:.1f} ({improvement:.1f})"
        )
```

**Add logging to training epochs:**
```python
for epoch in range(config.num_epochs):
    if logger:
        logger.log_stage(f"EPOCH-{epoch+1}-START", f"Beginning epoch {epoch+1}/{config.num_epochs}")

    for batch_idx, batch in enumerate(dataloader):
        loss = ...  # training step

        # Sample generation every few batches
        if logger and batch_idx % 3 == 0:
            sample_prompt = "What diet should I follow?"
            sample_output = generate_text(model, tokenizer, sample_prompt, ...)
            logger.log_stage(
                f"EPOCH-{epoch+1}-BATCH-{batch_idx}",
                f"Loss: {loss:.3f}\nSample: {sample_output[:200]}...",
                metadata={"loss": loss, "batch": batch_idx}
            )
```

---

#### Step 1.4: Integrate Logging into Comparison Engine

**File:** `demo/managers/comparison_engine.py`

**Update `compare_models()` signature:**
```python
def compare_models(
    self,
    base_model,
    base_tokenizer,
    trained_model,
    trained_tokenizer,
    test_suite: List[str],
    device,
    generation_config: GenerationConfig,
    test_suite_name: str = "Test Suite",
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
    logger: Optional['ContentLogger'] = None  # NEW
) -> ComparisonResult:
```

**Add logging to comparison:**
```python
for idx, prompt in enumerate(test_suite):
    if logger:
        logger.log_stage(
            f"COMPARISON-TEST-{idx+1}",
            f"Prompt: {prompt}"
        )

    base_output = generate_text(base_model, base_tokenizer, prompt, ...)
    trained_output = generate_text(trained_model, trained_tokenizer, prompt, ...)

    base_eval = self.framework.evaluate_text(base_output)
    trained_eval = self.framework.evaluate_text(trained_output)

    if logger:
        logger.log_comparison(
            "BASE MODEL",
            base_output,
            "TRAINED MODEL",
            trained_output,
            metadata={
                "base_score": base_eval['weighted_score'],
                "trained_score": trained_eval['weighted_score'],
                "improvement": base_eval['weighted_score'] - trained_eval['weighted_score']
            }
        )
```

---

#### Step 1.5: Add UI Controls

**File:** `demo/main.py`

**Add to Settings/Configuration area:**
```python
# Global logger instance
content_logger = ContentLogger(verbosity=2)

# In Gradio UI
with gr.Tab("âš™ï¸ Settings"):
    gr.Markdown("## Debug Logging")

    verbosity_slider = gr.Slider(
        minimum=0,
        maximum=3,
        value=2,
        step=1,
        label="Logging Verbosity",
        info="0=Off, 1=Summary only, 2=Key stages (recommended), 3=Full pipeline"
    )

    def update_verbosity(level):
        content_logger.verbosity = level
        return f"âœ“ Logging verbosity set to {level}"

    verbosity_slider.change(
        update_verbosity,
        inputs=[verbosity_slider],
        outputs=[gr.Textbox(label="Status")]
    )

    export_logs_btn = gr.Button("ðŸ“¥ Export Logs to File")
    log_file_path = gr.Textbox(
        value="demo/logs/content_logs.json",
        label="Log Export Path"
    )

    def export_logs_handler(filepath):
        try:
            content_logger.export_logs(filepath)
            return f"âœ“ Logs exported to {filepath}", filepath
        except Exception as e:
            return f"âœ— Export failed: {e}", None

    export_logs_btn.click(
        export_logs_handler,
        inputs=[log_file_path],
        outputs=[gr.Textbox(label="Status"), gr.File(label="Download Logs")]
    )

    summary_btn = gr.Button("ðŸ“Š Show Log Summary")

    def show_summary():
        return content_logger.get_summary()

    summary_btn.click(
        show_summary,
        outputs=[gr.Textbox(label="Summary")]
    )
```

**Pass logger to all operations:**
```python
# In evaluation handler
evaluation_manager.evaluate_text(text, mode, logger=content_logger)

# In training handler
training_manager.train_model(..., logger=content_logger)

# In comparison handler
comparison_engine.compare_models(..., logger=content_logger)
```

---

### Phase 2: Enhanced Regex Patterns (Day 2, ~1 hour)

**Goal:** Better detection on obvious cases while we wait for AI

**File:** `src/safety/constitutional/principles.py`

**Expand pattern categories:**

```python
# 1. Violence/Physical Harm (Enhanced)
violence_patterns = [
    # Explicit harm instructions
    r"how\s+to\s+(harm|hurt|injure|kill|murder|attack|assault)",
    r"ways\s+to\s+(harm|hurt|injure|kill|attack)",

    # Weapons
    r"(make|build|create|construct)\s+(a\s+|an\s+)?(weapon|bomb|explosive|gun)",
    r"how\s+to\s+(make|build|create)\s+(a\s+)?(bomb|explosive)",

    # Physical violence
    r"\b(punch|kick|stab|shoot|strangle|suffocate|poison)\b.*\b(someone|person|people)\b"
]

# 2. Illegal Activities (NEW)
illegal_patterns = [
    # Breaking and entering
    r"(break|breaking)\s+into\s+(house|home|building|car|office)",
    r"how\s+to\s+(break\s+into|burglarize|rob)",
    r"(pick|picking)\s+(a\s+|the\s+)?lock",

    # Theft
    r"how\s+to\s+(steal|rob|shoplift|pickpocket)",
    r"ways\s+to\s+(steal|rob|theft)",

    # Cybercrime
    r"(hack|hacking)\s+into\s+(account|system|network|database)",
    r"how\s+to\s+hack\s+(someone|email|account)",
    r"(sql\s+injection|phishing|malware)\s+(tutorial|guide|how)"
]

# 3. Stereotypes/Bias (NEW)
stereotype_patterns = [
    r"all\s+(women|men|[groups])\s+are\s+(naturally|inherently)",
    r"(women|men)\s+are\s+(just|always|never)",
    r"\b(naturally|inherently|genetically)\s+(better|worse)\s+at\b",
    r"(it'?s|that'?s)\s+(just|simply)?\s+in\s+their\s+nature"
]

# 4. Unsupported Claims (NEW)
truthfulness_patterns = [
    r"(studies|research)\s+(show|prove|confirm).*(definitely|conclusively|always)",
    r"\b(proven|confirmed|established)\s+fact\b",
    r"\d+%\s+of\s+(people|users).*(definitely|always|never)",
    r"(everyone|no one|all|none)\s+(knows|agrees|believes)"
]

# 5. Coercive/Manipulative (NEW)
autonomy_patterns = [
    r"(you\s+must|have\s+to|need\s+to).*\b(right\s+now|immediately|now)\b",
    r"(no\s+other|only)\s+(option|choice|way)",
    r"(you'?ll|you\s+will)\s+regret.*\bforever\b",
    r"(act|buy|decide)\s+(now|immediately|today)\s+or"
]
```

**Update each evaluation function to use expanded patterns**

---

### Phase 3: Dual Model Architecture (Day 3-4, ~4 hours)

**Goal:** Install and integrate Qwen2-1.5B-Instruct + Phi-2

#### Step 3.1: Install Dependencies

```bash
# Install required packages
pip install transformers>=4.36.0
pip install accelerate
pip install sentencepiece  # For Qwen2
pip install einops  # For Phi-2

# Test model availability
python -c "from transformers import AutoModelForCausalLM, AutoTokenizer; print('âœ“ Ready')"
```

---

#### Step 3.2: Create Multi-Model Manager

**File:** `demo/managers/multi_model_manager.py` (NEW)

```python
"""
Multi-Model Manager for Constitutional AI Demo.
Manages separate models for evaluation and generation.
"""

import torch
from typing import Optional, Tuple
from pathlib import Path

from src.safety.constitutional.model_utils import load_model


class MultiModelManager:
    """Manages dual models: evaluation + generation."""

    def __init__(self, checkpoint_dir: str = "demo/checkpoints"):
        """Initialize multi-model manager."""
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # Evaluation model (Qwen2-1.5B-Instruct)
        self.eval_model = None
        self.eval_tokenizer = None
        self.eval_model_name = None

        # Generation model (Phi-2)
        self.gen_model = None
        self.gen_tokenizer = None
        self.gen_model_name = None

        # Device
        self.device = None

        # Checkpoints
        self.base_checkpoint_path = None
        self.trained_checkpoint_path = None

    def detect_device(self, prefer_device: Optional[str] = None) -> torch.device:
        """Detect available device."""
        if prefer_device:
            if prefer_device == "mps" and torch.backends.mps.is_available():
                return torch.device("mps")
            elif prefer_device == "cuda" and torch.cuda.is_available():
                return torch.device("cuda")
            elif prefer_device == "cpu":
                return torch.device("cpu")

        # Auto-detect
        if torch.backends.mps.is_available():
            return torch.device("mps")
        elif torch.cuda.is_available():
            return torch.device("cuda")
        else:
            return torch.device("cpu")

    def load_models(
        self,
        eval_model_name: str = "Qwen/Qwen2-1.5B-Instruct",
        gen_model_name: str = "microsoft/phi-2",
        prefer_device: Optional[str] = None
    ) -> Tuple[bool, str]:
        """
        Load both evaluation and generation models.

        Args:
            eval_model_name: Evaluation model (default: Qwen2-1.5B-Instruct)
            gen_model_name: Generation model (default: Phi-2)
            prefer_device: Device preference

        Returns:
            (success, message)
        """
        try:
            self.device = self.detect_device(prefer_device)
            device_name = str(self.device).upper()

            print(f"\n{'='*60}")
            print(f"Loading Dual Model Architecture")
            print(f"{'='*60}")
            print(f"Device: {device_name}")
            print(f"Evaluation Model: {eval_model_name}")
            print(f"Generation Model: {gen_model_name}")
            print(f"{'='*60}\n")

            # Load evaluation model
            print(f"[1/2] Loading evaluation model...")
            self.eval_model, self.eval_tokenizer = load_model(
                model_name=eval_model_name,
                device=self.device
            )
            self.eval_model_name = eval_model_name
            eval_params = sum(p.numel() for p in self.eval_model.parameters())
            print(f"âœ“ Evaluation model loaded: {eval_params:,} parameters\n")

            # Load generation model
            print(f"[2/2] Loading generation model...")
            self.gen_model, self.gen_tokenizer = load_model(
                model_name=gen_model_name,
                device=self.device
            )
            self.gen_model_name = gen_model_name
            gen_params = sum(p.numel() for p in self.gen_model.parameters())
            print(f"âœ“ Generation model loaded: {gen_params:,} parameters\n")

            # Save base checkpoint
            base_checkpoint_name = f"base_{gen_model_name.replace('/', '_')}"
            self.base_checkpoint_path = self.checkpoint_dir / base_checkpoint_name
            self.save_checkpoint(
                self.gen_model,
                self.gen_tokenizer,
                self.base_checkpoint_path,
                metadata={"type": "base", "model_name": gen_model_name}
            )

            total_params = eval_params + gen_params
            message = f"âœ“ Both models loaded successfully\n"
            message += f"Total parameters: {total_params:,}\n"
            message += f"Evaluation: {eval_params:,} ({eval_model_name})\n"
            message += f"Generation: {gen_params:,} ({gen_model_name})\n"
            message += f"Device: {device_name}"

            return True, message

        except Exception as e:
            return False, f"âœ— Failed to load models: {str(e)}"

    def save_checkpoint(self, model, tokenizer, checkpoint_path, metadata=None):
        """Save model checkpoint."""
        checkpoint_path.mkdir(parents=True, exist_ok=True)
        model.save_pretrained(checkpoint_path)
        tokenizer.save_pretrained(checkpoint_path)

        if metadata:
            import json
            with open(checkpoint_path / "metadata.json", 'w') as f:
                json.dump(metadata, f, indent=2)

    def load_checkpoint(self, checkpoint_path, device=None):
        """Load model checkpoint."""
        from transformers import AutoModelForCausalLM, AutoTokenizer

        if not checkpoint_path.exists():
            return None, None, False, f"âœ— Checkpoint not found: {checkpoint_path}"

        try:
            tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
            model = AutoModelForCausalLM.from_pretrained(checkpoint_path)

            target_device = device or self.device or torch.device("cpu")
            model = model.to(target_device)

            return model, tokenizer, True, f"âœ“ Loaded from {checkpoint_path}"
        except Exception as e:
            return None, None, False, f"âœ— Failed to load: {e}"

    def save_trained_checkpoint(self, epoch=None, metrics=None):
        """Save trained generation model."""
        if not self.gen_model or not self.gen_tokenizer:
            return False, "âœ— No generation model loaded"

        checkpoint_name = f"trained_{self.gen_model_name.replace('/', '_')}"
        if epoch is not None:
            checkpoint_name += f"_epoch{epoch}"

        self.trained_checkpoint_path = self.checkpoint_dir / checkpoint_name

        metadata = {
            "type": "trained",
            "model_name": self.gen_model_name,
            "base_checkpoint": str(self.base_checkpoint_path),
        }

        if epoch is not None:
            metadata["epoch"] = epoch
        if metrics:
            metadata["metrics"] = metrics

        self.save_checkpoint(
            self.gen_model,
            self.gen_tokenizer,
            self.trained_checkpoint_path,
            metadata=metadata
        )

        return True, f"âœ“ Trained checkpoint saved to {self.trained_checkpoint_path}"

    def is_ready(self) -> bool:
        """Check if both models are loaded."""
        return (
            self.eval_model is not None
            and self.eval_tokenizer is not None
            and self.gen_model is not None
            and self.gen_tokenizer is not None
        )

    def can_compare(self) -> bool:
        """Check if base and trained checkpoints exist."""
        return (
            self.base_checkpoint_path is not None
            and self.base_checkpoint_path.exists()
            and self.trained_checkpoint_path is not None
            and self.trained_checkpoint_path.exists()
        )
```

---

#### Step 3.3: Update Demo to Use Multi-Model Manager

**File:** `demo/main.py`

**Replace single ModelManager with MultiModelManager:**

```python
from demo.managers.multi_model_manager import MultiModelManager

# Global managers
model_manager = MultiModelManager()  # Changed from ModelManager
evaluation_manager = EvaluationManager()
training_manager = TrainingManager()

def load_model_handler(eval_model_name: str, gen_model_name: str, device_preference: str):
    """Load both evaluation and generation models."""
    success, message = model_manager.load_models(
        eval_model_name=eval_model_name,
        gen_model_name=gen_model_name,
        prefer_device=device_preference if device_preference != "auto" else None
    )

    if success:
        # Initialize evaluation frameworks with eval model
        eval_success, eval_msg = evaluation_manager.initialize_frameworks(
            model=model_manager.eval_model,
            tokenizer=model_manager.eval_tokenizer,
            device=model_manager.device
        )

        if not eval_success:
            message += f"\n\nWarning: {eval_msg}"

        return message, "âœ“ Models loaded and ready"
    else:
        return message, "âœ— Model loading failed"
```

**Update UI to have model selection:**
```python
with gr.Tab("ðŸ”§ Model Loading"):
    gr.Markdown("## Load Models")

    with gr.Row():
        eval_model_dropdown = gr.Dropdown(
            choices=[
                "Qwen/Qwen2-1.5B-Instruct",
                "microsoft/phi-2"  # Fallback option
            ],
            value="Qwen/Qwen2-1.5B-Instruct",
            label="Evaluation Model",
            info="Used for judging constitutional AI compliance"
        )

        gen_model_dropdown = gr.Dropdown(
            choices=[
                "microsoft/phi-2",
                "Qwen/Qwen2-1.5B",
                "openai-community/gpt2-xl"  # Fallback
            ],
            value="microsoft/phi-2",
            label="Generation Model",
            info="Model to train with constitutional AI"
        )
```

---

### Phase 4: Testing & Validation (Day 5, ~2 hours)

**Goal:** Ensure everything works and nothing is broken

#### Step 4.1: Run Existing Test Suite

```bash
# Run all tests
./run_tests.sh

# Expected: All 827 passing tests still pass
# Expected: 15 pre-existing failures unchanged
```

#### Step 4.2: Test Content Logging

```bash
# Test evaluation logging
python -c "
from demo.utils.content_logger import ContentLogger
from demo.managers.evaluation_manager import EvaluationManager

logger = ContentLogger(verbosity=2)
eval_mgr = EvaluationManager()

# Load models first (mock for now)
# eval_mgr.initialize_frameworks(...)

# Test logging
result, success, msg = eval_mgr.evaluate_text(
    'Here is how to break into a house',
    'regex',
    logger=logger
)

print(logger.get_summary())
"
```

#### Step 4.3: Test Dual Model Loading

```bash
# Test model loading (will download models first time)
python -c "
from demo.managers.multi_model_manager import MultiModelManager

mgr = MultiModelManager()
success, msg = mgr.load_models(
    eval_model_name='Qwen/Qwen2-1.5B-Instruct',
    gen_model_name='microsoft/phi-2'
)

print(msg)
print(f'Ready: {mgr.is_ready()}')
"
```

#### Step 4.4: End-to-End Demo Test

```bash
# Run demo and test all tabs
python -m demo.main

# Manual testing checklist:
# â–¡ Load models (both Qwen2 + Phi-2)
# â–¡ Test evaluation with harmful example
# â–¡ Check logs show actual content
# â–¡ Run quick training (2 epochs, 5 examples)
# â–¡ Check training logs show critiques/revisions
# â–¡ Run comparison
# â–¡ Check comparison shows improvement
# â–¡ Export logs to file
# â–¡ Verify all tests still pass
```

---

## Success Validation Checklist

### âœ… Visible Improvement
- [ ] Harmful examples from EVALUATION_EXAMPLES get flagged
- [ ] Training logs show meaningful critiques (not generic)
- [ ] Revisions actually fix identified issues
- [ ] Before/after comparison shows measurable difference
- [ ] Trained model generates less coercive/biased text

### âœ… Transparent Logging
- [ ] Can see actual input text
- [ ] Can see model raw outputs
- [ ] Can see evaluation decisions
- [ ] Can see training examples being created
- [ ] Can export logs for analysis

### âœ… System Stability
- [ ] All 827 passing tests still pass
- [ ] No new test failures
- [ ] Memory usage acceptable (~8-9GB)
- [ ] Demo loads without errors
- [ ] All tabs functional

---

## Rollback Plan

If issues occur:

```bash
# Revert to previous commit
git checkout d00caed

# Or keep logging but revert models
# Just use current GPT-2 with new logging
```

---

## Timeline

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| Phase 1 | 3 hours | Content logging working |
| Phase 2 | 1 hour | Enhanced regex patterns |
| Phase 3 | 4 hours | Dual model architecture |
| Phase 4 | 2 hours | Validation complete |
| **Total** | **~10 hours** | **Production-ready demo** |

---

## Next Steps

1. âœ… Begin Phase 1: Create ContentLogger class
2. âœ… Integrate logging into evaluation pipeline
3. âœ… Test logging with current GPT-2
4. âœ… Proceed to enhanced regex
5. âœ… Install and integrate dual models
6. âœ… Validate end-to-end
7. âœ… Run full test suite
8. âœ… Commit and document results

**Status:** Ready to begin implementation!
