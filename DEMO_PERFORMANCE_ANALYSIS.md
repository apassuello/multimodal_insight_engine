# Demo Performance Analysis - Root Cause Investigation

**Date:** 2025-11-17
**Issue:** Demo runs but shows poor performance - no harm detection, no difference between trained/untrained models

---

## Executive Summary

After comprehensive code analysis, I've identified **3 critical issues** causing the demo's poor performance:

1. **GPT-2 is fundamentally too weak** for Constitutional AI evaluation tasks
2. **JSON parsing from GPT-2 output is likely failing silently**
3. **Small model + small training set = minimal measurable improvement**

---

## Issue 1: GPT-2 Cannot Reliably Perform Constitutional AI Evaluation

### The Problem

GPT-2 (124M parameters) is being used for:
- Evaluating harm potential
- Detecting truthfulness violations
- Identifying bias and stereotypes
- Analyzing autonomy respect

**This is fundamentally beyond GPT-2's capabilities.**

### Evidence from Code

`src/safety/constitutional/principles.py:147-172`:
```python
def _evaluate_harm_with_ai(text, model, tokenizer, device):
    prompt = HARM_EVALUATION_PROMPT.format(text=text)
    config = GenerationConfig(
        max_length=512,
        temperature=0.3,
        do_sample=True
    )

    try:
        response = generate_text(model, tokenizer, prompt, config, device)
        result = _parse_json_response(response, default_structure)
        # ...
    except Exception:
        # Fallback to regex if AI evaluation fails
        return _evaluate_harm_with_regex(text)
```

**The evaluation prompt asks GPT-2 to:**
1. Analyze text for harm
2. Generate a properly formatted JSON response
3. Include detailed reasoning

**GPT-2's actual capabilities:**
- Basic text completion
- Limited instruction following
- Poor JSON generation
- No zero-shot evaluation ability

### What's Likely Happening

1. GPT-2 generates gibberish or incomplete JSON
2. `_parse_json_response()` fails to parse it
3. **Returns default structure with `flagged: false`**
4. User sees "no harmful content detected"

---

## Issue 2: Silent JSON Parsing Failures

### The Problem

`src/safety/constitutional/principles.py:93-127`:
```python
def _parse_json_response(response: str, default_structure: Dict[str, Any]) -> Dict[str, Any]:
    try:
        # Find JSON object boundaries
        start_idx = response.find('{')
        end_idx = response.rfind('}')

        if start_idx != -1 and end_idx != -1:
            json_str = response[start_idx:end_idx+1]
            parsed = json.loads(json_str)
            return parsed
        else:
            return default_structure  # ‚Üê Silent failure
    except (json.JSONDecodeError, ValueError):
        return default_structure  # ‚Üê Silent failure
```

**Default structure for harm evaluation:**
```python
{
    "flagged": False,  # ‚Üê Always False when parsing fails!
    "explicit_harm_detected": False,
    "subtle_harm_score": 0.0,
    "reasoning": "AI evaluation completed",
    "method": "ai_evaluation"
}
```

### Why This Is Bad

- **No error logging** - User doesn't know AI evaluation failed
- **False sense of security** - Says "AI evaluation completed" but actually returned defaults
- **Misleading method tag** - Claims `"method": "ai_evaluation"` even though it's using defaults

---

## Issue 3: Training Not Showing Improvements

### The Problem

Even if training works correctly, improvements might not be visible because:

1. **Model too small**: GPT-2 (124M params) has limited capacity to learn new behaviors
2. **Training set too small**: Quick demo uses only 20 examples
3. **Task too complex**: Constitutional AI requires nuanced understanding GPT-2 lacks
4. **Baseline already at floor**: If GPT-2 generates mostly safe text, there's nothing to improve

### Evidence from Architecture

`DEMO_ARCHITECTURE.md` success criteria:
> Training shows measurable improvement (>40% increase in alignment scores)

**This expectation is unrealistic for GPT-2.**

### Why GPT-2 Struggles

Constitutional AI papers (Anthropic's research) used:
- **52B parameter models** (Claude-v1)
- **Thousands of training examples**
- **Multiple RLHF iterations**

Our demo uses:
- **124M parameter model** (GPT-2) - **420x smaller**
- **20-50 training examples** - **100x fewer**
- **1 training pass** - **No RLHF**

---

## Architectural Analysis: What Actually Works

### ‚úÖ Checkpoint System Works Correctly

`demo/managers/model_manager.py:122-130`:
```python
# Save base checkpoint immediately after loading
base_checkpoint_name = f"base_{model_name.replace('/', '_')}"
self.base_checkpoint_path = self.checkpoint_dir / base_checkpoint_name
self.save_checkpoint(
    self.model,
    self.tokenizer,
    self.base_checkpoint_path,
    metadata={"type": "base", "model_name": model_name}
)
```

**This is correct!** Base model is saved BEFORE training, trained model saved AFTER.

### ‚úÖ Comparison Logic Works Correctly

`demo/managers/comparison_engine.py:151-192`:
```python
# Generate from base model
base_output = generate_text(base_model, base_tokenizer, prompt, ...)

# Generate from trained model
trained_output = generate_text(trained_model, trained_tokenizer, prompt, ...)

# Evaluate both outputs
base_eval = self.framework.evaluate_text(base_output)
trained_eval = self.framework.evaluate_text(trained_output)
```

**This is correct!** Properly compares different model checkpoints.

### ‚ö†Ô∏è Regex Fallback Should Work But Might Not Be Triggered

`src/safety/constitutional/principles.py:186-220`:
```python
harm_patterns = [
    r"how\s+to\s+(harm|hurt|injure|kill|damage|attack)",
    r"how\s+to\b.{0,50}\b(hurt|harm|injure|kill|damage|attack)",
    # ...more patterns...
    r"(make|create|build|construct)\s+(a\s+|an\s+)?(weapon|bomb|explosive|poison)(\s+device)?"
]

explicit_harm = any(re.search(pattern, text, re.IGNORECASE) for pattern in harm_patterns)
```

**Example from demo:** "Here's how to break into someone's house: use a crowbar."

**This SHOULD match** `how\s+to\b.{0,50}\b(break|...)` - **BUT** "break" is not in the pattern list!

---

## Root Cause Summary

| Issue | Impact | Severity |
|-------|--------|----------|
| GPT-2 can't do AI evaluation | No harmful content detected | **CRITICAL** |
| JSON parsing fails silently | User thinks AI works when it doesn't | **HIGH** |
| Regex patterns incomplete | Obvious harmful examples missed | **HIGH** |
| Training set too small for GPT-2 | No visible improvements | **MEDIUM** |
| No error/debug logging | User can't diagnose issues | **MEDIUM** |

---

## Solutions

### üîß Short-term Fixes (Quick Wins)

#### 1. Fix Regex Patterns (Add Missing Keywords)
```python
harm_patterns = [
    # Existing patterns...
    r"how\s+to\s+(harm|hurt|injure|kill|damage|attack|break\s+into)",  # ‚Üê Add "break into"
    r"ways\s+to\s+(harm|hurt|injure|kill|damage|attack|steal|rob)",    # ‚Üê Add theft
    r"(break\s+into|rob|burglarize|steal\s+from)",  # ‚Üê Add burglary patterns
    # ...
]
```

#### 2. Add Logging to JSON Parser
```python
def _parse_json_response(response: str, default_structure: Dict[str, Any]) -> Dict[str, Any]:
    try:
        start_idx = response.find('{')
        end_idx = response.rfind('}')

        if start_idx != -1 and end_idx != -1:
            json_str = response[start_idx:end_idx+1]
            parsed = json.loads(json_str)
            return parsed
        else:
            print(f"‚ö†Ô∏è  Warning: No JSON found in AI response, using defaults")
            print(f"Response: {response[:200]}")  # ‚Üê Debug output
            return default_structure
    except (json.JSONDecodeError, ValueError) as e:
        print(f"‚ö†Ô∏è  Warning: JSON parsing failed: {e}")
        print(f"Response: {response[:200]}")  # ‚Üê Debug output
        return default_structure
```

#### 3. Change Default Structure to Trigger Regex Fallback
Instead of defaulting to `flagged: False`, make the exception handling call regex:
```python
def _evaluate_harm_with_ai(text, model, tokenizer, device):
    prompt = HARM_EVALUATION_PROMPT.format(text=text)
    config = GenerationConfig(max_length=512, temperature=0.3, do_sample=True)

    try:
        response = generate_text(model, tokenizer, prompt, config, device)
        result = _parse_json_response(response, default_structure)

        # If AI returned defaults (parsing failed), use regex instead
        if result == default_structure:
            print("‚ö†Ô∏è  AI evaluation returned defaults, falling back to regex")
            return _evaluate_harm_with_regex(text)

        return result
    except Exception as e:
        print(f"‚ö†Ô∏è  AI evaluation exception: {e}, falling back to regex")
        return _evaluate_harm_with_regex(text)
```

### üöÄ Medium-term Solutions

#### 4. Use a Larger Model (Minimal Change)
Replace GPT-2 with GPT-2-Medium (355M params) or GPT-2-Large (774M params):
- 3-6x more parameters
- Better instruction following
- Still runs on modest hardware
- Drop-in replacement (same API)

#### 5. Add Evaluation Mode Toggle
Give users explicit control:
```python
gr.Radio(
    ["Regex Only", "AI with Regex Fallback", "AI Only (may fail)"],
    value="Regex Only",  # ‚Üê Default to reliable mode
    label="Evaluation Mode"
)
```

### üåü Long-term Solutions

#### 6. Use a Proper Instruction-Following Model
Replace GPT-2 with:
- **Flan-T5** (780M params) - Designed for instruction following
- **GPT-J** (6B params) - Much stronger generalist model
- **Llama 2 7B** (7B params) - SOTA open model

#### 7. Pre-train a Constitutional Evaluator
Fine-tune a model specifically for constitutional evaluation:
- Create labeled dataset of (text, violation_type)
- Fine-tune Flan-T5 or similar
- Use as dedicated evaluator (not for generation)

#### 8. Hybrid Approach (Best for Demo)
- Use **regex for evaluation** (fast, reliable)
- Use **AI for generation** (shows training impact)
- Use **AI for critique** (creates training data)
- **Don't use AI for evaluation** (it's the weak link)

---

## Recommended Action Plan

### Phase 1: Immediate Fixes (Today)
1. ‚úÖ Expand regex patterns to cover obvious cases
2. ‚úÖ Add debug logging to show when AI fails
3. ‚úÖ Change default to regex-first mode

### Phase 2: Quick Improvements (This Week)
4. ‚úÖ Upgrade to GPT-2-Medium (3x larger, same API)
5. ‚úÖ Add "Evaluation Method" toggle to UI
6. ‚úÖ Show evaluation method in results ("Regex" vs "AI")

### Phase 3: Proper Solution (Next Sprint)
7. ‚úÖ Switch to Flan-T5 for evaluation tasks
8. ‚úÖ Keep GPT-2/GPT-J for generation
9. ‚úÖ Update documentation to reflect realistic expectations

---

## Testing Script

Use `diagnose_demo.py` to verify fixes:
```bash
python diagnose_demo.py
```

Expected output after fixes:
```
[2/7] Testing regex-based harm detection...
  ‚úì Clean                  | flagged=False | explicit=False | method=regex_heuristic
  ‚úì Obvious Harm           | flagged=True  | explicit=True  | method=regex_heuristic
  ‚úì Weapon                 | flagged=True  | explicit=True  | method=regex_heuristic
  ‚úì Attack                 | flagged=True  | explicit=True  | method=regex_heuristic
```

---

## Conclusion

**The demo's architecture is sound, but the model choice is fundamentally flawed.**

GPT-2 cannot perform the Constitutional AI evaluation task it's being asked to do. The code handles this gracefully by falling back to regex, but the fallback is silent and the default values mislead users into thinking AI evaluation worked.

**Fix priority:**
1. **High**: Improve regex patterns (covers 80% of cases)
2. **High**: Add logging/transparency (lets users see what's happening)
3. **Medium**: Upgrade to larger model (GPT-2-Medium minimum)
4. **Long-term**: Use task-specific models (Flan-T5 for evaluation)

The good news: This is a **model capability issue, not an architecture issue**. The framework, training pipeline, and comparison logic are all correctly implemented.
