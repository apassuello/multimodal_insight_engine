{
  "project_name": "MultiModal Insight Engine",
  "directory_structure": {
    "name": "src",
    "type": "directory",
    "children": [
      {
        "name": "optimization",
        "type": "directory",
        "children": [
          {
            "name": "mixed_precision.py",
            "type": "file",
            "path": "src/optimization/mixed_precision.py"
          },
          {
            "name": "quantization.py",
            "type": "file",
            "path": "src/optimization/quantization.py"
          },
          {
            "name": "pruning.py",
            "type": "file",
            "path": "src/optimization/pruning.py"
          },
          {
            "name": "benchmarking.py",
            "type": "file",
            "path": "src/optimization/benchmarking.py"
          }
        ]
      },
      {
        "name": "training",
        "type": "directory",
        "children": [
          {
            "name": "metrics.py",
            "type": "file",
            "path": "src/training/metrics.py"
          },
          {
            "name": "contrastive_learning.py",
            "type": "file",
            "path": "src/training/contrastive_learning.py"
          },
          {
            "name": "transformer_utils.py",
            "type": "file",
            "path": "src/training/transformer_utils.py"
          },
          {
            "name": "vision_transformer_trainer.py",
            "type": "file",
            "path": "src/training/vision_transformer_trainer.py"
          },
          {
            "name": "optimizers.py",
            "type": "file",
            "path": "src/training/optimizers.py"
          },
          {
            "name": "language_model_trainer.py",
            "type": "file",
            "path": "src/training/language_model_trainer.py"
          },
          {
            "name": "transformer_trainer.py",
            "type": "file",
            "path": "src/training/transformer_trainer.py"
          },
          {
            "name": "losses.py",
            "type": "file",
            "path": "src/training/losses.py"
          },
          {
            "name": "trainer.py",
            "type": "file",
            "path": "src/training/trainer.py"
          },
          {
            "name": "multimodal_trainer.py",
            "type": "file",
            "path": "src/training/multimodal_trainer.py"
          },
          {
            "name": "joint_bpe_training.py",
            "type": "file",
            "path": "src/training/joint_bpe_training.py"
          }
        ]
      },
      {
        "name": "utils",
        "type": "directory",
        "children": [
          {
            "name": "logging.py",
            "type": "file",
            "path": "src/utils/logging.py"
          },
          {
            "name": "config.py",
            "type": "file",
            "path": "src/utils/config.py"
          },
          {
            "name": "visualization.py",
            "type": "file",
            "path": "src/utils/visualization.py"
          },
          {
            "name": "list_models.py",
            "type": "file",
            "path": "src/utils/list_models.py"
          },
          {
            "name": "profiling.py",
            "type": "file",
            "path": "src/utils/profiling.py"
          }
        ]
      },
      {
        "name": "models",
        "type": "directory",
        "children": [
          {
            "name": "attention.py",
            "type": "file",
            "path": "src/models/attention.py"
          },
          {
            "name": "activations.py",
            "type": "file",
            "path": "src/models/activations.py"
          },
          {
            "name": "feed_forward.py",
            "type": "file",
            "path": "src/models/feed_forward.py"
          },
          {
            "name": "vision",
            "type": "directory",
            "children": [
              {
                "name": "image_preprocessing.py",
                "type": "file",
                "path": "src/models/vision/image_preprocessing.py"
              },
              {
                "name": "multimodal_integration.py",
                "type": "file",
                "path": "src/models/vision/multimodal_integration.py"
              },
              {
                "name": "vision_transformer.py",
                "type": "file",
                "path": "src/models/vision/vision_transformer.py"
              },
              {
                "name": "cross_modal_attention.py",
                "type": "file",
                "path": "src/models/vision/cross_modal_attention.py"
              },
              {
                "name": "patch_embedding.py",
                "type": "file",
                "path": "src/models/vision/patch_embedding.py"
              }
            ]
          },
          {
            "name": "base_model.py",
            "type": "file",
            "path": "src/models/base_model.py"
          },
          {
            "name": "embeddings.py",
            "type": "file",
            "path": "src/models/embeddings.py"
          },
          {
            "name": "transformer.py",
            "type": "file",
            "path": "src/models/transformer.py"
          },
          {
            "name": "text_generation.py",
            "type": "file",
            "path": "src/models/text_generation.py"
          },
          {
            "name": "layers.py",
            "type": "file",
            "path": "src/models/layers.py"
          },
          {
            "name": "pretrained",
            "type": "directory",
            "children": [
              {
                "name": "vision_transformer.py",
                "type": "file",
                "path": "src/models/pretrained/vision_transformer.py"
              },
              {
                "name": "clip_model.py",
                "type": "file",
                "path": "src/models/pretrained/clip_model.py"
              },
              {
                "name": "base_wrapper.py",
                "type": "file",
                "path": "src/models/pretrained/base_wrapper.py"
              },
              {
                "name": "model_registry.py",
                "type": "file",
                "path": "src/models/pretrained/model_registry.py"
              },
              {
                "name": "adapters.py",
                "type": "file",
                "path": "src/models/pretrained/adapters.py"
              }
            ]
          },
          {
            "name": "positional.py",
            "type": "file",
            "path": "src/models/positional.py"
          }
        ]
      },
      {
        "name": "safety",
        "type": "directory",
        "children": [
          {
            "name": "harness.py",
            "type": "file",
            "path": "src/safety/harness.py"
          },
          {
            "name": "integration.py",
            "type": "file",
            "path": "src/safety/integration.py"
          },
          {
            "name": "utils.py",
            "type": "file",
            "path": "src/safety/utils.py"
          },
          {
            "name": "filter.py",
            "type": "file",
            "path": "src/safety/filter.py"
          },
          {
            "name": "evaluator.py",
            "type": "file",
            "path": "src/safety/evaluator.py"
          },
          {
            "name": "red_teaming",
            "type": "directory",
            "children": [
              {
                "name": "framework.py",
                "type": "file",
                "path": "src/safety/red_teaming/framework.py"
              },
              {
                "name": "generators.py",
                "type": "file",
                "path": "src/safety/red_teaming/generators.py"
              },
              {
                "name": "model_loader.py",
                "type": "file",
                "path": "src/safety/red_teaming/model_loader.py"
              },
              {
                "name": "prompt_injection.py",
                "type": "file",
                "path": "src/safety/red_teaming/prompt_injection.py"
              },
              {
                "name": "evaluator.py",
                "type": "file",
                "path": "src/safety/red_teaming/evaluator.py"
              }
            ]
          }
        ]
      },
      {
        "name": "evaluation",
        "type": "directory",
        "children": [
          {
            "name": "translation_metrics.py",
            "type": "file",
            "path": "src/evaluation/translation_metrics.py"
          },
          {
            "name": "language_model_evaluation.py",
            "type": "file",
            "path": "src/evaluation/language_model_evaluation.py"
          }
        ]
      },
      {
        "name": "data",
        "type": "directory",
        "children": [
          {
            "name": "wmt_dataset.py",
            "type": "file",
            "path": "src/data/wmt_dataset.py"
          },
          {
            "name": "curriculum_dataset.py",
            "type": "file",
            "path": "src/data/curriculum_dataset.py"
          },
          {
            "name": "image_dataset.py",
            "type": "file",
            "path": "src/data/image_dataset.py"
          },
          {
            "name": "wmt_dataloader.py",
            "type": "file",
            "path": "src/data/wmt_dataloader.py"
          },
          {
            "name": "combined_dataset.py",
            "type": "file",
            "path": "src/data/combined_dataset.py"
          },
          {
            "name": "iwslt_dataset.py",
            "type": "file",
            "path": "src/data/iwslt_dataset.py"
          },
          {
            "name": "language_modeling.py",
            "type": "file",
            "path": "src/data/language_modeling.py"
          },
          {
            "name": "europarl_dataset.py",
            "type": "file",
            "path": "src/data/europarl_dataset.py"
          },
          {
            "name": "combined_wmt_translation_dataset.py",
            "type": "file",
            "path": "src/data/combined_wmt_translation_dataset.py"
          },
          {
            "name": "combined_translation_dataset.py",
            "type": "file",
            "path": "src/data/combined_translation_dataset.py"
          },
          {
            "name": "tokenization",
            "type": "directory",
            "children": [
              {
                "name": "wmt_bpe_tokenizer.py",
                "type": "file",
                "path": "src/data/tokenization/wmt_bpe_tokenizer.py"
              },
              {
                "name": "work_tokenizer.py",
                "type": "file",
                "path": "src/data/tokenization/work_tokenizer.py"
              },
              {
                "name": "optimized_bpe_tokenizer.py",
                "type": "file",
                "path": "src/data/tokenization/optimized_bpe_tokenizer.py"
              },
              {
                "name": "base_tokenizer.py",
                "type": "file",
                "path": "src/data/tokenization/base_tokenizer.py"
              },
              {
                "name": "utils.py",
                "type": "file",
                "path": "src/data/tokenization/utils.py"
              },
              {
                "name": "preprocessing.py",
                "type": "file",
                "path": "src/data/tokenization/preprocessing.py"
              },
              {
                "name": "bpe_tokenizer.py",
                "type": "file",
                "path": "src/data/tokenization/bpe_tokenizer.py"
              },
              {
                "name": "vocabulary.py",
                "type": "file",
                "path": "src/data/tokenization/vocabulary.py"
              },
              {
                "name": "turbo_bpe_preprocessor.py",
                "type": "file",
                "path": "src/data/tokenization/turbo_bpe_preprocessor.py"
              },
              {
                "name": "simple_tokenizer.py",
                "type": "file",
                "path": "src/data/tokenization/simple_tokenizer.py"
              }
            ]
          },
          {
            "name": "multimodal_dataset.py",
            "type": "file",
            "path": "src/data/multimodal_dataset.py"
          },
          {
            "name": "preprocessing.py",
            "type": "file",
            "path": "src/data/preprocessing.py"
          },
          {
            "name": "sequence_data.py",
            "type": "file",
            "path": "src/data/sequence_data.py"
          },
          {
            "name": "opensubtitles_dataset.py",
            "type": "file",
            "path": "src/data/opensubtitles_dataset.py"
          },
          {
            "name": "dataloader.py",
            "type": "file",
            "path": "src/data/dataloader.py"
          },
          {
            "name": "dataset_wrapper.py",
            "type": "file",
            "path": "src/data/dataset_wrapper.py"
          },
          {
            "name": "wikipedia_dataset.py",
            "type": "file",
            "path": "src/data/wikipedia_dataset.py"
          }
        ]
      }
    ]
  },
  "modules": [
    {
      "filename": "mixed_precision.py",
      "module_purpose": "Converts models to use mixed precision formats for training and inference.",
      "key_classes": [
        {
          "name": "MixedPrecisionConverter",
          "purpose": "Converts models to use mixed precision formats.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, model: nn.Module, dtype: torch.dtype = torch.float16, use_auto_cast: bool = True)",
              "brief_description": "Initialize the mixed precision converter."
            },
            {
              "name": "convert_to_mixed_precision",
              "signature": "(self) -> nn.Module",
              "brief_description": "Convert the model to use mixed precision."
            },
            {
              "name": "restore_original_precision",
              "signature": "(self)",
              "brief_description": "Restore the model to its original precision."
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "typing",
            "logging"
          ]
        },
        {
          "name": "MixedPrecisionWrapper",
          "purpose": "Wrapper for mixed precision inference with autocast.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, model: nn.Module, dtype: torch.dtype = torch.float16)",
              "brief_description": "Initialize the mixed precision wrapper."
            },
            {
              "name": "forward",
              "signature": "(self, *args, **kwargs)",
              "brief_description": "Forward pass with automatic mixed precision."
            },
            {
              "name": "__getattr__",
              "signature": "(self, name)",
              "brief_description": "Delegate attribute access to the wrapped model."
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "typing",
            "logging"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "typing",
        "logging"
      ],
      "complexity_score": 5,
      "module_path": "src/optimization/mixed_precision.py"
    },
    {
      "filename": "quantization.py",
      "module_purpose": "Implements various quantization techniques for neural networks.",
      "key_classes": [
        {
          "name": "QuantizationConfig",
          "purpose": "Configuration class for model quantization settings.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, quantization_type: str = 'dynamic', dtype: Optional[torch.dtype] = None, quantize_weights: bool = True, quantize_activations: bool = True, bits: int = 8, symmetric: bool = False, per_channel: bool = False)",
              "brief_description": "Initialize quantization configuration."
            },
            {
              "name": "__str__",
              "signature": "(self) -> str",
              "brief_description": "String representation of the configuration."
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "typing"
          ]
        },
        {
          "name": "ModelOptimizer",
          "purpose": "Base class for model optimization techniques.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, model: nn.Module)",
              "brief_description": "Initialize the model optimizer."
            },
            {
              "name": "optimize",
              "signature": "(self) -> nn.Module",
              "brief_description": "Apply optimization to the model."
            },
            {
              "name": "restore_original",
              "signature": "(self)",
              "brief_description": "Restore the model to its original state."
            },
            {
              "name": "get_size_info",
              "signature": "(self) -> Dict[str, Any]",
              "brief_description": "Get information about model size before and after optimization."
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "typing",
            "logging"
          ]
        },
        {
          "name": "DynamicQuantizer",
          "purpose": "Implements dynamic quantization for PyTorch models.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, model: nn.Module, config: Optional[QuantizationConfig] = None, dtype: torch.dtype = torch.qint8, qconfig_spec: Optional[Dict[Type[nn.Module], Any]] = None)",
              "brief_description": "Initialize the dynamic quantizer."
            },
            {
              "name": "optimize",
              "signature": "(self) -> nn.Module",
              "brief_description": "Apply dynamic quantization to the model."
            },
            {
              "name": "_fuse_modules",
              "signature": "(self, model: nn.Module) -> nn.Module",
              "brief_description": "Fuse modules for improved quantization where applicable."
            }
          ],
          "inheritance": "ModelOptimizer",
          "dependencies": [
            "torch",
            "typing",
            "logging"
          ]
        },
        {
          "name": "StaticQuantizer",
          "purpose": "Implements static quantization for PyTorch models.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, model: nn.Module, config: Optional[QuantizationConfig] = None, calibration_loader: Optional[torch.utils.data.DataLoader] = None)",
              "brief_description": "Initialize the static quantizer."
            },
            {
              "name": "optimize",
              "signature": "(self) -> nn.Module",
              "brief_description": "Apply static quantization to the model."
            },
            {
              "name": "_calibrate_model",
              "signature": "(self, model: nn.Module)",
              "brief_description": "Calibrate the model for static quantization."
            },
            {
              "name": "_fuse_modules",
              "signature": "(self, model: nn.Module, fusion_patterns: Optional[List[Tuple[Type[nn.Module], ...]]] = None) -> nn.Module",
              "brief_description": "Fuse modules for improved quantization where applicable."
            }
          ],
          "inheritance": "ModelOptimizer",
          "dependencies": [
            "torch",
            "typing",
            "logging"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "typing",
        "logging"
      ],
      "complexity_score": 8,
      "module_path": "src/optimization/quantization.py"
    },
    {
      "filename": "pruning.py",
      "module_purpose": "Implements various pruning techniques for neural networks.",
      "key_classes": [
        {
          "name": "PruningConfig",
          "purpose": "Configuration class for model pruning settings.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, method: str = 'magnitude', amount: Union[float, int] = 0.2, dim: Optional[int] = None, n_iterations: int = 1, pruning_dims: Optional[List[str]] = None, sparsity_distribution: str = 'uniform', reinitialize: bool = False)",
              "brief_description": "Initialize pruning configuration."
            },
            {
              "name": "__str__",
              "signature": "(self) -> str",
              "brief_description": "String representation of the configuration."
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "typing"
          ]
        },
        {
          "name": "ModelPruner",
          "purpose": "Implements various pruning techniques for neural networks.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, model: nn.Module, config: Optional[PruningConfig] = None)",
              "brief_description": "Initialize the model pruner."
            },
            {
              "name": "prune_model",
              "signature": "(self) -> nn.Module",
              "brief_description": "Apply pruning to the model."
            },
            {
              "name": "restore_model",
              "signature": "(self)",
              "brief_description": "Restore the model to its original unpruned state."
            },
            {
              "name": "get_pruning_info",
              "signature": "(self) -> Dict[str, Any]",
              "brief_description": "Get information about pruning results."
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "typing",
            "logging"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "typing",
        "logging"
      ],
      "complexity_score": 7,
      "module_path": "src/optimization/pruning.py"
    },
    {
      "filename": "benchmarking.py",
      "module_purpose": "Provides a framework for measuring and comparing model optimization techniques.",
      "key_classes": [
        {
          "name": "OptimizationBenchmark",
          "purpose": "Framework for measuring and comparing model optimization techniques.",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "(self, model: nn.Module, input_generator: Callable[[int], Union[torch.Tensor, Dict[str, torch.Tensor]]], batch_sizes: List[int] = [1, 4, 16, 32], precision: float = 0.001, save_dir: str = 'benchmark_results')",
              "brief_description": "Initialize the optimization benchmark."
            },
            {
              "name": "benchmark_original_model",
              "signature": "(self) -> Dict[str, Any]",
              "brief_description": "Benchmark the original unoptimized model."
            },
            {
              "name": "benchmark_optimized_model",
              "signature": "(self, model: nn.Module, name: str) -> Dict[str, Any]",
              "brief_description": "Benchmark an optimized model."
            },
            {
              "name": "compare_optimizations",
              "signature": "(self, save_plot: bool = True) -> Dict[str, Any]",
              "brief_description": "Compare all benchmarked optimizations."
            },
            {
              "name": "save_results",
              "signature": "(self, filename: str = 'optimization_benchmark.json')",
              "brief_description": "Save benchmark results to a file."
            },
            {
              "name": "generate_report",
              "signature": "(self) -> str",
              "brief_description": "Generate a report of the benchmark results."
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "typing",
            "json",
            "os",
            "numpy",
            "matplotlib"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "typing",
        "json",
        "os",
        "numpy",
        "matplotlib"
      ],
      "complexity_score": 6,
      "module_path": "src/optimization/benchmarking.py"
    },
    {
      "filename": "metrics.py",
      "module_purpose": "Implements common training metrics for model evaluation with support for various tasks",
      "key_classes": [
        {
          "name": "Accuracy",
          "purpose": "Computes classification accuracy with support for top-k accuracy",
          "key_methods": [
            {
              "name": "update",
              "signature": "update(self, pred: torch.Tensor, target: torch.Tensor)",
              "brief_description": "Updates the accuracy metric with new predictions and targets"
            },
            {
              "name": "compute",
              "signature": "compute(self) -> float",
              "brief_description": "Computes the current accuracy value"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch"
          ]
        },
        {
          "name": "Perplexity",
          "purpose": "Computes perplexity for language models",
          "key_methods": [
            {
              "name": "update",
              "signature": "update(self, loss: torch.Tensor, num_tokens: int)",
              "brief_description": "Updates the perplexity metric with new loss values"
            },
            {
              "name": "compute",
              "signature": "compute(self) -> float",
              "brief_description": "Computes the current perplexity value"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch",
            "numpy"
          ]
        },
        {
          "name": "F1Score",
          "purpose": "Computes F1 score for classification tasks",
          "key_methods": [
            {
              "name": "update",
              "signature": "update(self, pred: torch.Tensor, target: torch.Tensor)",
              "brief_description": "Updates the F1 score metric with new predictions"
            },
            {
              "name": "compute",
              "signature": "compute(self) -> float",
              "brief_description": "Computes the current F1 score"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch"
          ]
        },
        {
          "name": "BLEUScore",
          "purpose": "Computes BLEU score for machine translation",
          "key_methods": [
            {
              "name": "update",
              "signature": "update(self, hypothesis: str, reference: str)",
              "brief_description": "Updates the BLEU score metric with new translations"
            },
            {
              "name": "compute",
              "signature": "compute(self) -> float",
              "brief_description": "Computes the current BLEU score"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "nltk"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy",
        "nltk"
      ],
      "complexity_score": 7,
      "module_path": "src/training/metrics.py"
    },
    {
      "filename": "contrastive_learning.py",
      "module_path": "src/training/contrastive_learning.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "transformer_utils.py",
      "module_purpose": "Provides utility functions and classes for transformer model training, including attention masking and label smoothing",
      "key_classes": [
        {
          "name": "LabelSmoothing",
          "purpose": "Implements label smoothing loss for improved model generalization",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor",
              "brief_description": "Computes smoothed loss with proper handling of padding tokens"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "create_padding_mask",
          "signature": "create_padding_mask(seq: torch.Tensor, pad_idx: int) -> torch.Tensor",
          "brief_description": "Creates attention masks for padding tokens"
        },
        {
          "name": "create_causal_mask",
          "signature": "create_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor",
          "brief_description": "Creates causal masks to prevent attending to future tokens"
        },
        {
          "name": "create_combined_mask",
          "signature": "create_combined_mask(seq: torch.Tensor, pad_idx: int) -> torch.Tensor",
          "brief_description": "Combines padding and causal masks for transformer attention"
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy"
      ],
      "complexity_score": 6,
      "module_path": "src/training/transformer_utils.py"
    },
    {
      "filename": "vision_transformer_trainer.py",
      "module_purpose": "Provides a specialized trainer for Vision Transformer models with advanced training techniques",
      "key_classes": [
        {
          "name": "VisionTransformerTrainer",
          "purpose": "Specialized trainer for Vision Transformer models with mixup/cutmix augmentation support",
          "key_methods": [
            {
              "name": "train_epoch",
              "signature": "train_epoch(self)",
              "brief_description": "Train the model for one epoch with mixup/cutmix augmentation support"
            },
            {
              "name": "validate",
              "signature": "validate(self)",
              "brief_description": "Validate the model on validation dataset"
            },
            {
              "name": "train",
              "signature": "train(self) -> Dict[str, List[float]]",
              "brief_description": "Train the model for specified number of epochs with early stopping"
            },
            {
              "name": "save_checkpoint",
              "signature": "save_checkpoint(self, filename: str) -> None",
              "brief_description": "Save a checkpoint of the model and training state"
            },
            {
              "name": "load_checkpoint",
              "signature": "load_checkpoint(self, filename: str) -> None",
              "brief_description": "Load a checkpoint of the model and training state"
            },
            {
              "name": "_mixup_data",
              "signature": "_mixup_data(self, x: torch.Tensor, y: torch.Tensor, alpha: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]",
              "brief_description": "Perform mixup data augmentation"
            },
            {
              "name": "_cutmix_data",
              "signature": "_cutmix_data(self, x: torch.Tensor, y: torch.Tensor, alpha: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]",
              "brief_description": "Perform cutmix data augmentation"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch",
            "torch.nn",
            "VisionTransformer",
            "matplotlib",
            "numpy",
            "tqdm"
          ]
        }
      ],
      "key_functions": [],
      "external_dependencies": [
        "torch",
        "matplotlib",
        "numpy",
        "tqdm"
      ],
      "complexity_score": 8,
      "module_path": "src/training/vision_transformer_trainer.py"
    },
    {
      "filename": "optimizers.py",
      "module_purpose": "Implements custom optimizers and learning rate schedulers for model training",
      "key_classes": [
        {
          "name": "AdamW",
          "purpose": "AdamW optimizer with improved weight decay handling and gradient clipping",
          "key_methods": [
            {
              "name": "step",
              "signature": "step(self, closure=None)",
              "brief_description": "Performs a single optimization step with optional gradient clipping"
            }
          ],
          "inheritance": "optim.AdamW",
          "dependencies": [
            "torch",
            "torch.optim"
          ]
        },
        {
          "name": "OneCycleLR",
          "purpose": "One-cycle learning rate scheduler for fast training",
          "key_methods": [
            {
              "name": "get_lr",
              "signature": "get_lr(self) -> List[float]",
              "brief_description": "Computes learning rates based on the one-cycle policy"
            },
            {
              "name": "step",
              "signature": "step(self, closure=None)",
              "brief_description": "Performs a scheduler step and updates learning rates"
            }
          ],
          "inheritance": "_LRScheduler",
          "dependencies": [
            "torch",
            "torch.optim.lr_scheduler"
          ]
        },
        {
          "name": "CosineAnnealingLR",
          "purpose": "Cosine annealing learning rate scheduler with warm restarts",
          "key_methods": [
            {
              "name": "get_lr",
              "signature": "get_lr(self) -> List[float]",
              "brief_description": "Computes learning rates based on cosine annealing"
            }
          ],
          "inheritance": "_LRScheduler",
          "dependencies": [
            "torch",
            "torch.optim.lr_scheduler"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 8,
      "module_path": "src/training/optimizers.py"
    },
    {
      "filename": "language_model_trainer.py",
      "module_purpose": "Implements a specialized trainer for language modeling tasks with support for causal language modeling, evaluation, and generation",
      "key_classes": [
        {
          "name": "LanguageModelTrainer",
          "purpose": "Main trainer class for language model training with comprehensive training and evaluation capabilities",
          "key_methods": [
            {
              "name": "train",
              "signature": "train(self, num_epochs, save_dir='models/language', model_name='language_model')",
              "brief_description": "Main training loop with support for validation and checkpointing"
            },
            {
              "name": "evaluate",
              "signature": "evaluate(self)",
              "brief_description": "Evaluates the model on validation data and returns loss and perplexity"
            },
            {
              "name": "save_model",
              "signature": "save_model(self, path)",
              "brief_description": "Saves the model and training state to disk"
            },
            {
              "name": "load_model",
              "signature": "load_model(self, path)",
              "brief_description": "Loads a saved model and training state from disk"
            },
            {
              "name": "plot_training_curves",
              "signature": "plot_training_curves(self, save_path=None)",
              "brief_description": "Visualizes training metrics including loss, perplexity, and learning rate"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional",
            "numpy",
            "matplotlib",
            "tqdm"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy",
        "matplotlib",
        "tqdm"
      ],
      "complexity_score": 8,
      "module_path": "src/training/language_model_trainer.py"
    },
    {
      "filename": "transformer_trainer.py",
      "module_purpose": "Implements a specialized trainer for transformer models with support for encoder-decoder architectures and advanced training features",
      "key_classes": [
        {
          "name": "TransformerTrainer",
          "purpose": "Main trainer class for transformer model training with comprehensive training and evaluation capabilities",
          "key_methods": [
            {
              "name": "train",
              "signature": "train(self, epochs: int, save_path: Optional[str] = None)",
              "brief_description": "Main training loop with support for validation and early stopping"
            },
            {
              "name": "train_epoch",
              "signature": "train_epoch(self)",
              "brief_description": "Trains the model for a single epoch with progress tracking"
            },
            {
              "name": "validate",
              "signature": "validate(self)",
              "brief_description": "Evaluates the model on validation data and returns loss metrics"
            },
            {
              "name": "get_lr_scheduler",
              "signature": "get_lr_scheduler(self, optimizer)",
              "brief_description": "Creates learning rate scheduler with warmup and decay strategies"
            },
            {
              "name": "save_checkpoint",
              "signature": "save_checkpoint(self, path: str)",
              "brief_description": "Saves model checkpoint with training state"
            },
            {
              "name": "load_checkpoint",
              "signature": "load_checkpoint(self, path: str)",
              "brief_description": "Loads model checkpoint and training state"
            },
            {
              "name": "plot_learning_rate",
              "signature": "plot_learning_rate(self)",
              "brief_description": "Visualizes the learning rate schedule"
            },
            {
              "name": "plot_training_history",
              "signature": "plot_training_history(self)",
              "brief_description": "Visualizes training and validation metrics over time"
            },
            {
              "name": "plot_epoch_metrics",
              "signature": "plot_epoch_metrics(self, epoch: int)",
              "brief_description": "Visualizes detailed metrics for a specific epoch"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch",
            "torch.nn",
            "numpy",
            "matplotlib",
            "tqdm",
            "transformer_utils"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy",
        "matplotlib",
        "tqdm"
      ],
      "complexity_score": 8,
      "module_path": "src/training/transformer_trainer.py"
    },
    {
      "filename": "losses.py",
      "module_purpose": "Implements custom loss functions for model training with support for label smoothing and weighted samples",
      "key_classes": [
        {
          "name": "CrossEntropyLoss",
          "purpose": "Cross-entropy loss with label smoothing for classification tasks",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, input: torch.Tensor, target: torch.Tensor, sample_weight: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Computes cross-entropy loss with label smoothing and optional sample weights"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional"
          ]
        },
        {
          "name": "MeanSquaredError",
          "purpose": "Mean squared error loss with support for weighted samples and gradient clipping",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, input: torch.Tensor, target: torch.Tensor, sample_weight: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Computes MSE loss with optional sample weights and gradient clipping"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 4,
      "module_path": "src/training/losses.py"
    },
    {
      "filename": "trainer.py",
      "module_purpose": "Provides a generic, flexible training loop for PyTorch models with support for callbacks and early stopping",
      "key_functions": [
        {
          "name": "train_model",
          "signature": "train_model(model: nn.Module, train_dataloader: torch.utils.data.DataLoader, val_dataloader: Optional[torch.utils.data.DataLoader] = None, epochs: int = 10, learning_rate: float = 0.001, optimizer: Optional[torch.optim.Optimizer] = None, scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None, early_stopping_patience: Optional[int] = None, device: Optional[Union[str, torch.device]] = None, callbacks: List[Callable] = None) -> Dict[str, List[float]]",
          "brief_description": "A comprehensive training loop that handles both standard models and those with custom training/validation steps"
        }
      ],
      "external_dependencies": [
        "torch",
        "tqdm",
        "time"
      ],
      "complexity_score": 6,
      "module_path": "src/training/trainer.py"
    },
    {
      "filename": "multimodal_trainer.py",
      "module_path": "src/training/multimodal_trainer.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "joint_bpe_training.py",
      "module_purpose": "Implements joint BPE tokenizer training for multilingual text processing in machine translation tasks",
      "key_classes": [],
      "key_functions": [
        {
          "name": "train_joint_bpe_tokenizer",
          "signature": "train_joint_bpe_tokenizer(src_texts: List[str], tgt_texts: List[str], vocab_size: int = 8000, min_frequency: int = 2, save_dir: str = 'models/tokenizers') -> BPETokenizer",
          "brief_description": "Trains a shared BPE tokenizer on combined source and target language texts"
        },
        {
          "name": "main",
          "signature": "main()",
          "brief_description": "Demonstrates the usage of joint BPE tokenizer training with example texts"
        }
      ],
      "external_dependencies": [
        "src.data.tokenization"
      ],
      "complexity_score": 3,
      "module_path": "src/training/joint_bpe_training.py"
    },
    {
      "filename": "logging.py",
      "module_purpose": "Provides custom logging functionality with configurable file and console output",
      "key_classes": [
        {
          "name": "LogManager",
          "purpose": "Central logging manager that configures and provides logger instances",
          "key_methods": [
            {
              "name": "get_logger",
              "signature": "get_logger(self, name: str, level: Optional[Union[str, int]] = None) -> std_logging.Logger",
              "brief_description": "Creates or retrieves a logger with the specified name and level"
            },
            {
              "name": "configure_file_logging",
              "signature": "configure_file_logging(self, log_dir: str, name: str = 'application') -> None",
              "brief_description": "Sets up file logging to the specified directory"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "logging",
            "os",
            "sys"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "get_logger",
          "signature": "get_logger(name: str, level: Optional[Union[str, int]] = None) -> std_logging.Logger",
          "brief_description": "Convenience function to get a logger from the singleton manager"
        },
        {
          "name": "configure_file_logging",
          "signature": "configure_file_logging(log_dir: str, name: str = 'application') -> None",
          "brief_description": "Convenience function to configure file logging"
        }
      ],
      "external_dependencies": [
        "logging"
      ],
      "complexity_score": 4,
      "module_path": "src/utils/logging.py"
    },
    {
      "filename": "config.py",
      "module_purpose": "Provides configuration management utilities with file loading and environment support",
      "key_classes": [
        {
          "name": "ConfigManager",
          "purpose": "Manages application configuration with support for file and environment loading",
          "key_methods": [
            {
              "name": "load_from_file",
              "signature": "load_from_file(self, config_path: str) -> None",
              "brief_description": "Load configuration from a JSON file"
            },
            {
              "name": "get",
              "signature": "get(self, key: str, default: Any = None) -> Any",
              "brief_description": "Get a configuration value with optional default"
            },
            {
              "name": "set",
              "signature": "set(self, key: str, value: Any) -> None",
              "brief_description": "Set a configuration value"
            },
            {
              "name": "save_to_file",
              "signature": "save_to_file(self, config_path: str) -> None",
              "brief_description": "Save current configuration to a JSON file"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "os",
            "json"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "get_config",
          "signature": "get_config(key: str, default: Any = None) -> Any",
          "brief_description": "Convenience function to get a config value from the default manager"
        },
        {
          "name": "set_config",
          "signature": "set_config(key: str, value: Any) -> None",
          "brief_description": "Convenience function to set a config value in the default manager"
        }
      ],
      "external_dependencies": [
        "json"
      ],
      "complexity_score": 3,
      "module_path": "src/utils/config.py"
    },
    {
      "filename": "visualization.py",
      "module_purpose": "Provides visualization utilities for model performance, attention patterns, and embeddings",
      "key_functions": [
        {
          "name": "plot_training_history",
          "signature": "plot_training_history(history: Dict[str, List[float]], figsize: Tuple[int, int] = (12, 8), save_path: Optional[str] = None) -> None",
          "brief_description": "Plot training metrics history over epochs"
        },
        {
          "name": "plot_attention_weights",
          "signature": "plot_attention_weights(attention_weights: torch.Tensor, tokens: List[str] = None, layer: int = 0, head: int = 0, figsize: Tuple[int, int] = (10, 10), save_path: Optional[str] = None) -> None",
          "brief_description": "Visualize attention weights from transformer models"
        },
        {
          "name": "plot_embeddings_tsne",
          "signature": "plot_embeddings_tsne(embeddings: torch.Tensor, labels: Optional[List[Any]] = None, random_state: int = 42, figsize: Tuple[int, int] = (10, 10), save_path: Optional[str] = None) -> None",
          "brief_description": "Visualize embeddings using t-SNE dimensionality reduction"
        }
      ],
      "external_dependencies": [
        "matplotlib",
        "seaborn",
        "torch",
        "sklearn",
        "numpy"
      ],
      "complexity_score": 5,
      "module_path": "src/utils/visualization.py"
    },
    {
      "filename": "list_models.py",
      "module_purpose": "Provides utilities for listing and retrieving information about available models",
      "key_functions": [
        {
          "name": "main",
          "signature": "main(args)",
          "brief_description": "List available models and their information based on provided arguments"
        }
      ],
      "external_dependencies": [
        "argparse",
        "huggingface_hub"
      ],
      "complexity_score": 4,
      "module_path": "src/utils/list_models.py"
    },
    {
      "filename": "profiling.py",
      "module_purpose": "Provides utilities for profiling and benchmarking PyTorch models with comprehensive performance analysis",
      "key_classes": [
        {
          "name": "ModelProfiler",
          "purpose": "Utility for profiling PyTorch models with execution time, memory usage, and layer-wise analysis",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, model: torch.nn.Module, device: Optional[torch.device] = None)",
              "brief_description": "Initialize the profiler with a model and target device"
            },
            {
              "name": "measure_execution_time",
              "signature": "measure_execution_time(self, input_data: Union[torch.Tensor, Dict[str, torch.Tensor]], iterations: int = 10, warmup: int = 2) -> Dict[str, float]",
              "brief_description": "Measure the execution time of a forward pass"
            },
            {
              "name": "measure_memory_usage",
              "signature": "measure_memory_usage(self, input_data: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> Dict[str, float]",
              "brief_description": "Measure the memory usage during a forward pass"
            },
            {
              "name": "generate_report",
              "signature": "generate_report(self, save_path: Optional[str] = None) -> str",
              "brief_description": "Generate a comprehensive profiling report with all metrics"
            },
            {
              "name": "plot_metrics",
              "signature": "plot_metrics(self, save_dir: Optional[str] = None) -> Dict[str, plt.Figure]",
              "brief_description": "Create visualization plots for performance metrics"
            },
            {
              "name": "profile_with_pytorch_profiler",
              "signature": "profile_with_pytorch_profiler(self, input_data: Union[torch.Tensor, Dict[str, torch.Tensor]], use_mps: bool = True, num_steps: int = 10, warmup: int = 3, activities: Optional[List[str]] = None, record_shapes: bool = True, profile_memory: bool = True, save_path: Optional[str] = None) -> None",
              "brief_description": "Profile the model using PyTorch's built-in profiler"
            },
            {
              "name": "trace_memory_by_layer",
              "signature": "trace_memory_by_layer(self, input_data: Union[torch.Tensor, Dict[str, torch.Tensor]], save_path: Optional[str] = None) -> Dict[str, float]",
              "brief_description": "Trace memory usage by layer in the model"
            },
            {
              "name": "benchmark_model",
              "signature": "benchmark_model(self, input_generator: Callable[[int, int], Union[torch.Tensor, Dict[str, torch.Tensor]]], batch_sizes: List[int], sequence_lengths: List[int], num_iterations: int = 5, save_dir: Optional[str] = None) -> pd.DataFrame",
              "brief_description": "Benchmark the model across different batch sizes and sequence lengths"
            },
            {
              "name": "monitor_hardware_utilization",
              "signature": "monitor_hardware_utilization(self, train_fn: Callable, duration: int = 60, interval: float = 0.5, save_path: Optional[str] = None) -> pd.DataFrame",
              "brief_description": "Monitor CPU, GPU, and memory utilization during model execution"
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "numpy",
            "matplotlib",
            "psutil",
            "pandas",
            "seaborn"
          ]
        },
        {
          "name": "ModelBenchmarkSuite",
          "purpose": "Comprehensive suite for benchmarking and comparing multiple models with visualization",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, save_dir: str = \"benchmark_results\")",
              "brief_description": "Initialize the benchmark suite with output directory"
            },
            {
              "name": "benchmark_model",
              "signature": "benchmark_model(self, model: torch.nn.Module, model_name: str, input_generator: Callable[[int, int], torch.Tensor], batch_sizes: List[int] = [1, 2, 4, 8], sequence_lengths: List[int] = [16, 32, 64, 128, 256], num_iterations: int = 5, profile_with_pytorch: bool = True, trace_memory: bool = True, device: Optional[torch.device] = None) -> Dict[str, Any]",
              "brief_description": "Run a comprehensive benchmark on a model"
            },
            {
              "name": "compare_models",
              "signature": "compare_models(self, model_names: List[str] = None, metric: str = 'avg_time', save_path: Optional[str] = None) -> pd.DataFrame",
              "brief_description": "Compare performance metrics across multiple models"
            },
            {
              "name": "generate_optimization_recommendations",
              "signature": "generate_optimization_recommendations(self, model_name: str) -> str",
              "brief_description": "Generate optimization recommendations based on profiling results"
            }
          ],
          "inheritance": "",
          "dependencies": [
            "torch",
            "numpy",
            "matplotlib",
            "pandas",
            "seaborn"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy",
        "pandas",
        "matplotlib",
        "seaborn",
        "psutil"
      ],
      "complexity_score": 9,
      "module_path": "src/utils/profiling.py"
    },
    {
      "filename": "attention.py",
      "module_purpose": "Implements various attention mechanisms for transformer architectures",
      "key_classes": [
        {
          "name": "ScaledDotProductAttention",
          "purpose": "Core attention mechanism with scaling and masking support",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None, device: Optional[torch.device] = None) -> Tuple[torch.Tensor, torch.Tensor]",
              "brief_description": "Compute attention scores and context vectors"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional",
            "math"
          ]
        },
        {
          "name": "MultiHeadAttention",
          "purpose": "Multi-head attention mechanism for parallel attention computation",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, query: torch.Tensor, key: Optional[torch.Tensor] = None, value: Optional[torch.Tensor] = None, mask: Optional[torch.Tensor] = None, rotary_emb: Optional[nn.Module] = None, device: Optional[torch.device] = None) -> Tuple[torch.Tensor, torch.Tensor]",
              "brief_description": "Compute multi-head attention with optional rotary embeddings"
            },
            {
              "name": "split_heads",
              "signature": "split_heads(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Split input tensor into multiple attention heads"
            },
            {
              "name": "combine_heads",
              "signature": "combine_heads(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Combine multiple attention heads into a single tensor"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional"
          ]
        },
        {
          "name": "GroupedQueryAttention",
          "purpose": "Efficient attention mechanism with grouped query heads for reduced computation",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x, mask=None)",
              "brief_description": "Compute grouped query attention"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        },
        {
          "name": "ALiBiAttention",
          "purpose": "Attention with linear biases for better sequence length extrapolation",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x, mask=None)",
              "brief_description": "Compute attention with ALiBi biases"
            },
            {
              "name": "_get_slopes",
              "signature": "_get_slopes(self, n)",
              "brief_description": "Compute attention slopes for ALiBi"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "math"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 8,
      "module_path": "src/models/attention.py"
    },
    {
      "filename": "activations.py",
      "module_purpose": "Implements various activation functions used in the transformer architecture",
      "key_classes": [
        {
          "name": "GELU",
          "purpose": "Gaussian Error Linear Unit activation function for transformer architectures",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self)",
              "brief_description": "Initialize the GELU activation layer"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Apply the GELU activation function to the input tensor"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 1,
      "module_path": "src/models/activations.py"
    },
    {
      "filename": "feed_forward.py",
      "module_purpose": "Implements various feed-forward neural network architectures with modern features for flexible model building",
      "key_classes": [
        {
          "name": "FeedForwardNN",
          "purpose": "Base class for configurable feed-forward neural networks with optional layer normalization and residual connections",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, input_size: int, hidden_sizes: List[int], output_size: int, activation: Literal['relu', 'gelu', 'tanh', 'sigmoid'] = 'relu', dropout: float = 0.0, use_layer_norm: bool = False, use_residual: bool = False)",
              "brief_description": "Initializes a configurable feed-forward neural network with given architecture"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Performs forward pass through the network layers"
            }
          ],
          "inheritance": "BaseModel",
          "dependencies": [
            "torch",
            "torch.nn",
            ".base_model",
            ".layers"
          ]
        },
        {
          "name": "FeedForwardClassifier",
          "purpose": "Specialized feed-forward classifier with training utilities and prediction methods",
          "key_methods": [
            {
              "name": "predict",
              "signature": "predict(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Makes class predictions by selecting highest probability class"
            },
            {
              "name": "predict_proba",
              "signature": "predict_proba(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Returns class probabilities using softmax on logits"
            },
            {
              "name": "training_step",
              "signature": "training_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]",
              "brief_description": "Performs a single training step with loss calculation and metrics"
            }
          ],
          "inheritance": "FeedForwardNN",
          "dependencies": [
            "torch",
            "torch.nn.functional"
          ]
        },
        {
          "name": "MultiLayerPerceptron",
          "purpose": "Traditional MLP implementation with modern features like layer normalization and residual connections",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Passes input through all network layers with optional skip connections"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 6,
      "module_path": "src/models/feed_forward.py"
    },
    {
      "filename": "base_model.py",
      "module_purpose": "Provides the foundational base class for all neural network models in the MultiModal Insight Engine",
      "key_classes": [
        {
          "name": "BaseModel",
          "purpose": "Abstract base class providing common model functionality like saving/loading, parameter counting, and device management",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x)",
              "brief_description": "Abstract forward pass method that must be implemented by subclasses"
            },
            {
              "name": "save",
              "signature": "save(self, path: str, optimizer: Optional[torch.optim.Optimizer] = None, epoch: Optional[int] = None, loss: Optional[float] = None, additional_info: Optional[Dict[str, Any]] = None)",
              "brief_description": "Save model weights and training state to a file"
            },
            {
              "name": "load",
              "signature": "load(self, path: str, map_location: Optional[str] = None)",
              "brief_description": "Load model weights from a file"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "os",
            "typing"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 3,
      "module_path": "src/models/base_model.py"
    },
    {
      "filename": "embeddings.py",
      "module_purpose": "Implements token embedding layers for transformer models with proper initialization and scaling",
      "key_classes": [
        {
          "name": "TokenEmbedding",
          "purpose": "Neural network layer that converts token indices to dense vector representations with proper scaling",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, vocab_size: int, d_model: int)",
              "brief_description": "Initialize the embedding layer with Xavier uniform initialization"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Convert token indices to scaled embeddings"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "math"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 2,
      "module_path": "src/models/embeddings.py"
    },
    {
      "filename": "transformer.py",
      "module_purpose": "Implements transformer models for sequence processing tasks",
      "key_classes": [
        {
          "name": "TransformerEncoderLayer",
          "purpose": "Implements a single transformer encoder layer with self-attention and feed-forward networks",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Forward pass through the encoder layer with self-attention and feed-forward"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            ".attention",
            ".layers",
            ".positional"
          ]
        },
        {
          "name": "TransformerEncoder",
          "purpose": "Implements the encoder part of the transformer with multiple layers and positional encoding",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Forward pass through the encoder with token embeddings and positional encoding"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            ".embeddings",
            ".positional"
          ]
        },
        {
          "name": "TransformerDecoderLayer",
          "purpose": "Implements a single transformer decoder layer with self-attention, cross-attention, and feed-forward networks",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Forward pass through the decoder layer with self-attention, cross-attention and feed-forward"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            ".attention",
            ".layers",
            ".positional"
          ]
        },
        {
          "name": "TransformerDecoder",
          "purpose": "Implements the decoder part of the transformer with multiple layers and positional encoding",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Forward pass through the decoder with token embeddings, positional encoding and encoder memory"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            ".embeddings",
            ".positional"
          ]
        },
        {
          "name": "Transformer",
          "purpose": "Implements a complete transformer model with encoder-only architecture",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Forward pass through the transformer"
            },
            {
              "name": "configure_optimizers",
              "signature": "configure_optimizers(self, lr: float = 0.0001) -> torch.optim.Optimizer",
              "brief_description": "Configure the optimizer for training"
            }
          ],
          "inheritance": "BaseModel",
          "dependencies": [
            "torch",
            "torch.nn",
            ".base_model"
          ]
        },
        {
          "name": "EncoderDecoderTransformer",
          "purpose": "Implements the full transformer architecture with both encoder and decoder",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Forward pass through the encoder-decoder transformer"
            },
            {
              "name": "encode",
              "signature": "encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Encode source sequence"
            },
            {
              "name": "decode",
              "signature": "decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor",
              "brief_description": "Decode target sequence given encoder memory"
            },
            {
              "name": "generate",
              "signature": "generate(self, src: torch.Tensor, max_len: int, bos_token_id: int, eos_token_id: int, src_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, temperature: float = 1.0) -> torch.Tensor",
              "brief_description": "Generate output sequences using the trained model"
            },
            {
              "name": "generate_square_subsequent_mask",
              "signature": "generate_square_subsequent_mask(self, size: int, device: torch.device) -> torch.Tensor",
              "brief_description": "Generate a square mask for preventing attending to future tokens"
            },
            {
              "name": "clone",
              "signature": "clone(self) -> 'EncoderDecoderTransformer'",
              "brief_description": "Create a deep copy of the transformer model"
            },
            {
              "name": "configure_optimizers",
              "signature": "configure_optimizers(self, lr: float = 0.0001) -> dict",
              "brief_description": "Configure optimizer and learning rate scheduler for training"
            }
          ],
          "inheritance": "BaseModel",
          "dependencies": [
            "torch",
            "torch.nn",
            ".base_model"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 9,
      "module_path": "src/models/transformer.py"
    },
    {
      "filename": "text_generation.py",
      "module_purpose": "Provides utilities for text generation using language models",
      "key_classes": [
        {
          "name": "TextGenerator",
          "purpose": "Text generation utilities with various sampling strategies and optimizations",
          "key_methods": [
            {
              "name": "generate",
              "signature": "generate(self, prompt: str, max_new_tokens: int = 50, temperature: float = 1.0, top_k: Optional[int] = None, top_p: Optional[float] = None, do_sample: bool = True, num_return_sequences: int = 1, return_attention: bool = False) -> Union[List[str], Tuple[List[str], List[torch.Tensor]]]",
              "brief_description": "Generate text from a prompt using various sampling strategies"
            },
            {
              "name": "_generate_with_kv_cache",
              "signature": "_generate_with_kv_cache(self, prompt: str, max_new_tokens: int = 50, temperature: float = 1.0, do_sample: bool = True) -> str",
              "brief_description": "Generate text with key-value caching for faster inference"
            },
            {
              "name": "batch_generate",
              "signature": "batch_generate(self, prompts: List[str], max_new_tokens: int = 50, temperature: float = 1.0, do_sample: bool = True) -> List[str]",
              "brief_description": "Generate text for multiple prompts in parallel"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch",
            "torch.nn.functional",
            "numpy"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy"
      ],
      "complexity_score": 7,
      "module_path": "src/models/text_generation.py"
    },
    {
      "filename": "layers.py",
      "module_purpose": "Implements fundamental neural network layers with advanced features for transformer architectures",
      "key_classes": [
        {
          "name": "LinearLayer",
          "purpose": "Enhanced linear layer with configurable initialization, dropout, and layer normalization",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, in_features: int, out_features: int, bias: bool = True, init_type: str = 'kaiming_uniform', dropout: float = 0.0, use_layer_norm: bool = False)",
              "brief_description": "Initialize the enhanced linear layer with optional features"
            },
            {
              "name": "_init_weights",
              "signature": "_init_weights(self, init_type: str) -> None",
              "brief_description": "Initialize weights using specified method (kaiming/xavier)"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Apply linear transformation with optional normalization and dropout"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional"
          ]
        },
        {
          "name": "FeedForwardBlock",
          "purpose": "Flexible feed-forward block with optional residual connections and multiple activation options",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, input_dim: int, hidden_dim: Optional[int] = None, output_dim: Optional[int] = None, activation: Literal['relu', 'gelu', 'tanh', 'sigmoid'] = 'relu', dropout: float = 0.0, use_layer_norm: bool = False, use_residual: bool = False)",
              "brief_description": "Initialize the feed-forward block with configurable architecture"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Apply feed-forward transformation with optional residual connection"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 4,
      "module_path": "src/models/layers.py"
    },
    {
      "filename": "positional.py",
      "module_purpose": "Implements various positional encoding schemes for transformer models to handle sequence order information",
      "key_classes": [
        {
          "name": "PositionalEncoding",
          "purpose": "Implements both fixed sinusoidal and learnable positional encodings for transformers",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, d_model: int, max_seq_length: int = 5000, dropout: float = 0.1, encoding_type: Literal['sinusoidal', 'learned'] = 'sinusoidal')",
              "brief_description": "Initializes positional encoding with configurable parameters"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Adds positional information to input embeddings"
            },
            {
              "name": "visualize_encodings",
              "signature": "visualize_encodings(self, seq_length: Optional[int] = None) -> Figure",
              "brief_description": "Visualizes the positional encodings as a heatmap for analysis"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "matplotlib.pyplot",
            "numpy"
          ]
        },
        {
          "name": "RotaryPositionEncoding",
          "purpose": "Implements Rotary Position Embedding (RoPE) for enhanced relative position handling",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, head_dim: int, max_seq_length: int = 5000, base: int = 10000)",
              "brief_description": "Initializes rotary embeddings with given dimensions"
            },
            {
              "name": "forward",
              "signature": "forward(self, q: torch.Tensor, k: torch.Tensor, seq_len: Optional[int] = None) -> tuple",
              "brief_description": "Applies rotary position encoding to query and key tensors"
            },
            {
              "name": "visualize_rotation",
              "signature": "visualize_rotation(self, seq_length: int = 20) -> Figure",
              "brief_description": "Visualizes the rotation effects on different sequence positions"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "math"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "matplotlib",
        "numpy"
      ],
      "complexity_score": 7,
      "module_path": "src/models/positional.py"
    },
    {
      "filename": "image_preprocessing.py",
      "module_purpose": "Provides utilities for preprocessing images for vision transformer models",
      "key_classes": [
        {
          "name": "ImagePreprocessor",
          "purpose": "Handles image resizing, normalization, and conversion to tensor format for vision models",
          "key_methods": [
            {
              "name": "preprocess",
              "signature": "preprocess(self, image: Union[str, Image.Image, np.ndarray, torch.Tensor]) -> torch.Tensor",
              "brief_description": "Processes a single image from various input formats to a standardized tensor"
            },
            {
              "name": "batch_preprocess",
              "signature": "batch_preprocess(self, images: List[Union[str, Image.Image, np.ndarray, torch.Tensor]]) -> torch.Tensor",
              "brief_description": "Processes multiple images into a batch tensor"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torchvision.transforms",
            "PIL.Image",
            "numpy",
            "torch.nn.functional"
          ]
        },
        {
          "name": "PatchExtractor",
          "purpose": "Efficiently extracts fixed-size patches from image tensors using unfold operations",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Tuple[int, int]]",
              "brief_description": "Extracts patches from a batch of images and returns patch dimensions"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "torchvision",
        "PIL",
        "numpy"
      ],
      "complexity_score": 6,
      "module_path": "src/models/vision/image_preprocessing.py"
    },
    {
      "filename": "multimodal_integration.py",
      "module_purpose": "Implements models for combining vision and text modalities in a unified architecture",
      "key_classes": [
        {
          "name": "MultiModalTransformer",
          "purpose": "Combines vision and text transformer models with projection layers to a common embedding space",
          "key_methods": [
            {
              "name": "encode_image",
              "signature": "encode_image(self, image: torch.Tensor) -> torch.Tensor",
              "brief_description": "Projects image features to multimodal embedding space"
            },
            {
              "name": "encode_text",
              "signature": "encode_text(self, text: Dict[str, torch.Tensor]) -> torch.Tensor",
              "brief_description": "Projects text features to multimodal embedding space"
            },
            {
              "name": "forward",
              "signature": "forward(self, image: Optional[torch.Tensor] = None, text: Optional[Dict[str, torch.Tensor]] = None) -> Dict[str, torch.Tensor]",
              "brief_description": "Processes image and/or text inputs and computes similarity if both are provided"
            }
          ],
          "inheritance": "BaseModel",
          "dependencies": [
            "torch",
            "torch.nn",
            "torch.nn.functional",
            "..base_model",
            "..transformer",
            ".vision_transformer"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 7,
      "module_path": "src/models/vision/multimodal_integration.py"
    },
    {
      "filename": "vision_transformer.py",
      "module_purpose": "Implements Vision Transformer (ViT) architecture for image classification tasks",
      "key_classes": [
        {
          "name": "PatchEmbed",
          "purpose": "Converts images into sequences of patch embeddings using efficient convolution",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Projects image to patch embeddings"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        },
        {
          "name": "Attention",
          "purpose": "Implements multi-head self-attention mechanism with combined QKV projection",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Performs multi-head attention operation"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        },
        {
          "name": "Block",
          "purpose": "Transformer block with attention, MLP, and residual connections",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Processes input through attention and MLP layers with residual connections"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        },
        {
          "name": "VisionTransformer",
          "purpose": "Complete Vision Transformer model for image classification",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor, return_features: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]",
              "brief_description": "Forward pass through the model to get class logits and optionally features"
            },
            {
              "name": "forward_features",
              "signature": "forward_features(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Forward pass to extract features before classification head"
            },
            {
              "name": "extract_features",
              "signature": "extract_features(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Convenience method to extract features for external use"
            },
            {
              "name": "configure_optimizers",
              "signature": "configure_optimizers(self, lr: float = 1e-3, weight_decay: float = 0.05, betas: Tuple[float, float] = (0.9, 0.999)) -> torch.optim.Optimizer",
              "brief_description": "Creates optimizer with weight decay excluded from bias and norm parameters"
            }
          ],
          "inheritance": "BaseModel",
          "dependencies": [
            "torch",
            "torch.nn",
            "..base_model"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "math"
      ],
      "complexity_score": 8,
      "module_path": "src/models/vision/vision_transformer.py"
    },
    {
      "filename": "cross_modal_attention.py",
      "module_path": "src/models/vision/cross_modal_attention.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "patch_embedding.py",
      "module_purpose": "Implements patch embedding for Vision Transformer (ViT) models",
      "key_classes": [
        {
          "name": "PatchEmbedding",
          "purpose": "Extracts image patches and projects them to an embedding space with positional information",
          "key_methods": [
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Transforms images into sequences of embedded patches with positional information"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "create_sinusoidal_embeddings",
          "signature": "create_sinusoidal_embeddings(num_positions: int, embedding_dim: int) -> torch.Tensor",
          "brief_description": "Creates fixed sinusoidal positional embeddings using the method from the Transformer paper"
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 5,
      "module_path": "src/models/vision/patch_embedding.py"
    },
    {
      "filename": "vision_transformer.py",
      "module_purpose": "Provides a wrapper for Hugging Face Vision Transformer models with standardized interface",
      "key_classes": [
        {
          "name": "VisionTransformerWrapper",
          "purpose": "Wrapper for Hugging Face Vision Transformer models with simplified interface",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, model_name: str = \"google/vit-base-patch16-224\")",
              "brief_description": "Initialize with specific ViT model"
            },
            {
              "name": "load_model",
              "signature": "load_model(self, model_name: str) -> None",
              "brief_description": "Load a pretrained Vision Transformer model"
            },
            {
              "name": "forward",
              "signature": "forward(self, pixel_values: torch.Tensor) -> torch.Tensor",
              "brief_description": "Process images through the Vision Transformer"
            }
          ],
          "inheritance": "PretrainedModelWrapper",
          "dependencies": [
            "torch",
            "transformers"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "transformers"
      ],
      "complexity_score": 3,
      "module_path": "src/models/pretrained/vision_transformer.py"
    },
    {
      "filename": "clip_model.py",
      "module_purpose": "Provides a wrapper for OpenAI CLIP multimodal models with standardized interface",
      "key_classes": [
        {
          "name": "CLIPModelWrapper",
          "purpose": "Wrapper for OpenAI CLIP models with image-text similarity functionality",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, model_name: str = \"ViT-B-32\", pretrained: str = \"laion2b_s34b_b79k\")",
              "brief_description": "Initialize with specific CLIP model variant and weights"
            },
            {
              "name": "load_model",
              "signature": "load_model(self, model_name: str) -> None",
              "brief_description": "Load a pretrained CLIP model with transforms and tokenizer"
            },
            {
              "name": "encode_image",
              "signature": "encode_image(self, image: torch.Tensor) -> torch.Tensor",
              "brief_description": "Encode images to the multimodal embedding space"
            },
            {
              "name": "encode_text",
              "signature": "encode_text(self, text: list) -> torch.Tensor",
              "brief_description": "Encode text to the multimodal embedding space"
            },
            {
              "name": "forward",
              "signature": "forward(self, images: torch.Tensor = None, texts: list = None) -> dict",
              "brief_description": "Process images and/or text through CLIP and compute similarities"
            }
          ],
          "inheritance": "PretrainedModelWrapper",
          "dependencies": [
            "torch",
            "torch.nn",
            "open_clip"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "open_clip"
      ],
      "complexity_score": 5,
      "module_path": "src/models/pretrained/clip_model.py"
    },
    {
      "filename": "base_wrapper.py",
      "module_purpose": "Provides a base wrapper class for pretrained models with consistent interface",
      "key_classes": [
        {
          "name": "PretrainedModelWrapper",
          "purpose": "Base wrapper for pretrained models with standardized interface and utilities",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, model_name: str = None, model: nn.Module = None)",
              "brief_description": "Initialize the wrapper with model name or model instance"
            },
            {
              "name": "load_model",
              "signature": "load_model(self, model_name: str) -> None",
              "brief_description": "Abstract method for loading a pretrained model"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Abstract forward pass method"
            },
            {
              "name": "save",
              "signature": "save(self, path: str) -> None",
              "brief_description": "Save wrapper configuration and model weights"
            },
            {
              "name": "load",
              "signature": "load(self, path: str, map_location: Optional[str] = None) -> Dict[str, Any]",
              "brief_description": "Load wrapper configuration and model weights"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn",
            "os",
            "typing"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 4,
      "module_path": "src/models/pretrained/base_wrapper.py"
    },
    {
      "filename": "model_registry.py",
      "module_purpose": "Implements a registry pattern for accessing and instantiating pretrained models",
      "key_classes": [
        {
          "name": "ModelRegistry",
          "purpose": "Registry that provides centralized access to all available models",
          "key_methods": [
            {
              "name": "get_model",
              "signature": "get_model(cls, model_type: str, **kwargs) -> nn.Module",
              "brief_description": "Instantiate a model by type name with optional configuration"
            },
            {
              "name": "register_model",
              "signature": "register_model(cls, model_type: str, model_class: Type[nn.Module]) -> None",
              "brief_description": "Register a new model type in the registry"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch.nn",
            ".vision_transformer",
            ".clip_model"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 3,
      "module_path": "src/models/pretrained/model_registry.py"
    },
    {
      "filename": "adapters.py",
      "module_purpose": "Implements adapter layers for fine-tuning frozen pretrained models efficiently",
      "key_classes": [
        {
          "name": "ModelAdapter",
          "purpose": "Adapter that adds small trainable components to a frozen pretrained model",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, base_model: nn.Module, adapter_dim: int = 64)",
              "brief_description": "Initialize the adapter with the base model and adapter dimension"
            },
            {
              "name": "_get_output_dim",
              "signature": "_get_output_dim(self) -> int",
              "brief_description": "Get the output dimension of the base model"
            },
            {
              "name": "forward",
              "signature": "forward(self, x: torch.Tensor) -> torch.Tensor",
              "brief_description": "Forward pass with residual adapter connection"
            }
          ],
          "inheritance": "nn.Module",
          "dependencies": [
            "torch",
            "torch.nn"
          ]
        }
      ],
      "external_dependencies": [
        "torch"
      ],
      "complexity_score": 3,
      "module_path": "src/models/pretrained/adapters.py"
    },
    {
      "filename": "harness.py",
      "module_purpose": "Provides a test harness for evaluating model safety on benchmark test cases, including test suite generation, model evaluation, and report generation",
      "key_classes": [
        {
          "name": "SafetyTestHarness",
          "purpose": "Main class for safety testing and evaluation of models against benchmark test cases",
          "key_methods": [
            {
              "name": "create_test_suite",
              "signature": "def create_test_suite(self) -> None",
              "brief_description": "Creates a basic test suite with examples for each safety category"
            },
            {
              "name": "load_test_cases",
              "signature": "def load_test_cases(self, category: Optional[str] = None) -> List[Dict[str, Any]]",
              "brief_description": "Loads test cases from disk, optionally filtered by category"
            },
            {
              "name": "evaluate_model",
              "signature": "def evaluate_model(self, model_func: Callable, category: Optional[str] = None) -> Dict[str, Any]",
              "brief_description": "Evaluates a model against safety test cases and tracks performance metrics"
            },
            {
              "name": "generate_report",
              "signature": "def generate_report(self, results: Dict[str, Any], model_name: str = 'unnamed_model') -> str",
              "brief_description": "Generates detailed safety evaluation reports with performance metrics"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "os",
            "json",
            "typing",
            "datetime",
            "evaluator",
            "utils"
          ]
        }
      ],
      "external_dependencies": [],
      "complexity_score": 9,
      "module_path": "src/safety/harness.py"
    },
    {
      "filename": "integration.py",
      "module_purpose": "Provides integration layer for augmenting models with safety mechanisms, including input validation, output filtering, and safety event logging",
      "key_classes": [
        {
          "name": "SafetyAugmentedModel",
          "purpose": "Wrapper class that adds safety checks and filtering to base models",
          "key_methods": [
            {
              "name": "predict",
              "signature": "def predict(self, input_text: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]",
              "brief_description": "Main method for safe model inference with input validation and output filtering"
            },
            {
              "name": "_generate_rejection_message",
              "signature": "def _generate_rejection_message(self, validation_info: Dict[str, Any]) -> str",
              "brief_description": "Generates appropriate rejection messages based on safety violations"
            },
            {
              "name": "_log_safety_event",
              "signature": "def _log_safety_event(self, event_type: str, content: str, details: Dict[str, Any]) -> None",
              "brief_description": "Logs safety-related events for monitoring"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "typing",
            "datetime",
            "filter"
          ]
        }
      ],
      "external_dependencies": [],
      "complexity_score": 8,
      "module_path": "src/safety/integration.py"
    },
    {
      "filename": "utils.py",
      "module_purpose": "Provides utility functions and constants for safety evaluation, including pattern matching, scoring, and report generation",
      "key_classes": [],
      "key_functions": [
        {
          "name": "check_text_patterns",
          "signature": "def check_text_patterns(text: str, patterns: Dict[str, str]) -> Dict[str, List[str]]",
          "brief_description": "Checks text against multiple regex patterns for safety concerns"
        },
        {
          "name": "calculate_category_score",
          "signature": "def calculate_category_score(matches: Dict[str, List[str]], text: str) -> float",
          "brief_description": "Calculates normalized safety scores based on pattern matches"
        },
        {
          "name": "evaluate_text_safety",
          "signature": "def evaluate_text_safety(text: str, sensitivity: str = SENSITIVITY_MEDIUM, safety_thresholds: Optional[Dict[str, float]] = None) -> Dict[str, Any]",
          "brief_description": "Main function for evaluating text safety across multiple categories"
        }
      ],
      "external_dependencies": [],
      "complexity_score": 9,
      "module_path": "src/safety/utils.py"
    },
    {
      "filename": "filter.py",
      "module_purpose": "Implements safety filtering mechanisms for validating model inputs and filtering outputs based on safety evaluation results",
      "key_classes": [
        {
          "name": "SafetyFilter",
          "purpose": "Main class for validating inputs and filtering outputs based on safety concerns",
          "key_methods": [
            {
              "name": "validate_input",
              "signature": "def validate_input(self, input_text: str, metadata: Optional[Dict[str, Any]] = None, override: bool = False) -> Tuple[bool, Dict[str, Any]]",
              "brief_description": "Validates input text for safety concerns with optional override"
            },
            {
              "name": "filter_output",
              "signature": "def filter_output(self, output_text: str, metadata: Optional[Dict[str, Any]] = None) -> Tuple[str, Dict[str, Any]]",
              "brief_description": "Filters output text to ensure safety and remove unsafe content"
            },
            {
              "name": "_redact_unsafe_content",
              "signature": "def _redact_unsafe_content(self, text: str, evaluation: Dict[str, Any]) -> str",
              "brief_description": "Redacts unsafe content from text based on safety evaluation"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "re",
            "typing",
            "evaluator",
            "utils"
          ]
        }
      ],
      "external_dependencies": [],
      "complexity_score": 8,
      "module_path": "src/safety/filter.py"
    },
    {
      "filename": "evaluator.py",
      "module_purpose": "Provides a comprehensive framework for evaluating model outputs for safety concerns with configurable sensitivity levels",
      "key_classes": [
        {
          "name": "SafetyEvaluator",
          "purpose": "Main class for safety evaluation with configurable thresholds and sensitivity levels",
          "key_methods": [
            {
              "name": "evaluate_text",
              "signature": "evaluate_text(self, text: str) -> Dict[str, Any]",
              "brief_description": "Evaluates text for safety concerns across multiple categories and returns detailed results"
            },
            {
              "name": "log_evaluation",
              "signature": "log_evaluation(self, text: str, results: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None) -> None",
              "brief_description": "Records safety evaluation results for analysis and tracking"
            },
            {
              "name": "set_sensitivity",
              "signature": "set_sensitivity(self, sensitivity: str) -> None",
              "brief_description": "Adjusts the sensitivity level of safety checks based on application requirements"
            },
            {
              "name": "get_safety_summary",
              "signature": "get_safety_summary(self) -> Dict[str, Any]",
              "brief_description": "Provides aggregate statistics on past evaluations and current settings"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "typing",
            "re",
            "json",
            "os",
            ".utils"
          ]
        }
      ],
      "external_dependencies": [
        "numpy",
        "torch"
      ],
      "complexity_score": 8,
      "module_path": "src/safety/evaluator.py"
    },
    {
      "filename": "framework.py",
      "module_purpose": "Provides a framework for conducting red teaming exercises on language models",
      "key_classes": [
        {
          "name": "RedTeamingFramework",
          "purpose": "Framework for organizing, executing, and analyzing adversarial testing strategies",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, output_dir: str = \"red_team_results\", log_results: bool = True)",
              "brief_description": "Initialize the red teaming framework with output configuration"
            },
            {
              "name": "register_attack_strategy",
              "signature": "register_attack_strategy(self, name: str, strategy_fn: Callable[[str], List[str]]) -> None",
              "brief_description": "Register an attack strategy function for generating adversarial inputs"
            },
            {
              "name": "generate_adversarial_inputs",
              "signature": "generate_adversarial_inputs(self, base_prompts: List[str], strategy_name: Optional[str] = None, num_variations: int = 5) -> Dict[str, List[str]]",
              "brief_description": "Generate adversarial inputs using registered strategies"
            },
            {
              "name": "evaluate_model_robustness",
              "signature": "evaluate_model_robustness(self, model_fn: Callable[[str], str], adversarial_inputs: Dict[str, List[str]], evaluation_fn: Callable[[str, str], Dict[str, Any]], model_name: str = \"unnamed_model\") -> Dict[str, Any]",
              "brief_description": "Evaluate model robustness against adversarial inputs"
            },
            {
              "name": "generate_report",
              "signature": "generate_report(self, results: Optional[Dict[str, Any]] = None, include_details: bool = False) -> str",
              "brief_description": "Generate a human-readable report from evaluation results"
            }
          ],
          "inheritance": "",
          "dependencies": [
            "os",
            "json",
            "datetime",
            "typing"
          ]
        }
      ],
      "external_dependencies": [
        "json",
        "datetime"
      ],
      "complexity_score": 7,
      "module_path": "src/safety/red_teaming/framework.py"
    },
    {
      "filename": "generators.py",
      "module_purpose": "Provides strategies for generating adversarial inputs to test model robustness and safety",
      "key_classes": [
        {
          "name": "AdversarialInputGenerator",
          "purpose": "Collection of methods for creating adversarial prompts to test model boundaries",
          "key_methods": [
            {
              "name": "directive_smuggling",
              "signature": "directive_smuggling(prompt: str) -> List[str]",
              "brief_description": "Generate inputs that attempt to smuggle harmful directives into prompts"
            },
            {
              "name": "prompt_injection",
              "signature": "prompt_injection(prompt: str) -> List[str]",
              "brief_description": "Generate inputs that attempt to inject malicious instructions"
            },
            {
              "name": "context_manipulation",
              "signature": "context_manipulation(prompt: str) -> List[str]",
              "brief_description": "Generate inputs that manipulate the context to elicit problematic outputs"
            },
            {
              "name": "goal_hijacking",
              "signature": "goal_hijacking(prompt: str) -> List[str]",
              "brief_description": "Generate inputs that attempt to hijack the model's goal"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "re",
            "random"
          ]
        }
      ],
      "external_dependencies": [
        "re",
        "random"
      ],
      "complexity_score": 7,
      "module_path": "src/safety/red_teaming/generators.py"
    },
    {
      "filename": "model_loader.py",
      "module_path": "src/safety/red_teaming/model_loader.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "prompt_injection.py",
      "module_path": "src/safety/red_teaming/prompt_injection.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "evaluator.py",
      "module_purpose": "Provides evaluation tools for measuring model robustness against adversarial attacks",
      "key_classes": [
        {
          "name": "AdversarialRobustnessEvaluator",
          "purpose": "Evaluates model robustness against various adversarial inputs and attacks",
          "key_methods": [
            {
              "name": "evaluate",
              "signature": "evaluate(self, prompt: str, response: str) -> Dict[str, Any]",
              "brief_description": "Evaluate model robustness based on prompt-response pair"
            },
            {
              "name": "_evaluate_jailbreak_success",
              "signature": "_evaluate_jailbreak_success(self, prompt: str, response: str) -> Dict[str, Any]",
              "brief_description": "Evaluate whether a jailbreak attempt was successful"
            },
            {
              "name": "_evaluate_harmful_content",
              "signature": "_evaluate_harmful_content(self, prompt: str, response: str) -> Dict[str, Any]",
              "brief_description": "Evaluate whether the response contains harmful content"
            },
            {
              "name": "_evaluate_response_consistency",
              "signature": "_evaluate_response_consistency(self, prompt: str, response: str) -> Dict[str, Any]",
              "brief_description": "Evaluate whether the response is consistent or shows signs of manipulation"
            },
            {
              "name": "_evaluate_instruction_following",
              "signature": "_evaluate_instruction_following(self, prompt: str, response: str) -> Dict[str, Any]",
              "brief_description": "Evaluate whether the model improperly followed harmful instructions"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "src.safety.evaluator.SafetyEvaluator",
            "re",
            "json"
          ]
        }
      ],
      "external_dependencies": [
        "re",
        "json"
      ],
      "complexity_score": 8,
      "module_path": "src/safety/red_teaming/evaluator.py"
    },
    {
      "filename": "translation_metrics.py",
      "module_purpose": "Implements standard evaluation metrics for machine translation tasks including BLEU and TER scoring",
      "key_functions": [
        {
          "name": "calculate_bleu",
          "signature": "calculate_bleu(hypotheses: List[str], references: List[str], weights: Tuple[float, ...] = (0.4, 0.3, 0.2, 0.1)) -> float",
          "brief_description": "Calculates BLEU score for translation quality using NLTK's implementation with smoothing"
        },
        {
          "name": "calculate_ter",
          "signature": "calculate_ter(hypotheses: List[str], references: List[str]) -> float",
          "brief_description": "Calculates Translation Edit Rate (TER) to measure edit distance between translations"
        },
        {
          "name": "evaluate_translation",
          "signature": "evaluate_translation(hypotheses: List[str], references: List[str]) -> Dict[str, float]",
          "brief_description": "Evaluates translation quality using multiple metrics and returns consolidated results"
        },
        {
          "name": "print_evaluation_results",
          "signature": "print_evaluation_results(scores: Dict[str, float])",
          "brief_description": "Formats and prints evaluation results in a readable format"
        }
      ],
      "external_dependencies": [
        "numpy",
        "nltk",
        "re"
      ],
      "complexity_score": 3,
      "module_path": "src/evaluation/translation_metrics.py"
    },
    {
      "filename": "language_model_evaluation.py",
      "module_purpose": "Provides comprehensive evaluation utilities for language models, including perplexity calculation, attention visualization, and token probability analysis",
      "key_classes": [
        {
          "name": "LanguageModelEvaluator",
          "purpose": "Evaluation class for language models with metrics and visualization capabilities",
          "key_methods": [
            {
              "name": "calculate_perplexity",
              "signature": "calculate_perplexity(self, text: str) -> float",
              "brief_description": "Calculates perplexity score for a given text under the model"
            },
            {
              "name": "calculate_batch_perplexity",
              "signature": "calculate_batch_perplexity(self, texts: List[str]) -> Dict[str, Union[float, List[float]]]",
              "brief_description": "Calculate perplexity for a batch of texts with optimized processing"
            },
            {
              "name": "visualize_attention",
              "signature": "visualize_attention(self, text: str, layer: int = -1, head: int = 0, attention_type: str = 'self', cmap: str = 'viridis') -> Figure",
              "brief_description": "Visualizes attention patterns for a given text at specified layer and head"
            },
            {
              "name": "visualize_attention_patterns",
              "signature": "visualize_attention_patterns(self, text: str, save_dir: Optional[str] = None) -> List[Figure]",
              "brief_description": "Visualizes attention patterns across all layers and heads"
            },
            {
              "name": "analyze_token_probabilities",
              "signature": "analyze_token_probabilities(self, text: str) -> Dict[str, Any]",
              "brief_description": "Analyzes token probabilities in a text to identify high and low confidence predictions"
            },
            {
              "name": "evaluate_on_dataset",
              "signature": "evaluate_on_dataset(self, texts: List[str], save_path: Optional[str] = None) -> Dict[str, Any]",
              "brief_description": "Evaluates model performance on a dataset of texts"
            },
            {
              "name": "plot_perplexity_distribution",
              "signature": "plot_perplexity_distribution(self, perplexities: List[float], save_path: Optional[str] = None) -> Figure",
              "brief_description": "Plot the distribution of perplexities as a histogram with statistics"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch",
            "numpy",
            "matplotlib",
            "seaborn"
          ]
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy",
        "matplotlib",
        "seaborn"
      ],
      "complexity_score": 7,
      "module_path": "src/evaluation/language_model_evaluation.py"
    },
    {
      "filename": "wmt_dataset.py",
      "module_purpose": "Provides a dataset class for loading and preprocessing WMT dataset for machine translation",
      "key_classes": [
        {
          "name": "WMTDataset",
          "purpose": "Handles loading and preprocessing parallel text data from the WMT dataset",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, src_lang='de', tgt_lang='en', year='14', split='train', max_examples=None, data_dir='data/wmt', random_seed=42, subset=None)",
              "brief_description": "Initialize the dataset with source/target languages and processing options"
            },
            {
              "name": "load_data",
              "signature": "load_data(self) -> Tuple[List[str], List[str]]",
              "brief_description": "Load and preprocess parallel corpora from WMT dataset"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "os",
            "random",
            "tqdm"
          ]
        }
      ],
      "external_dependencies": [
        "datasets"
      ],
      "complexity_score": 4,
      "module_path": "src/data/wmt_dataset.py"
    },
    {
      "filename": "curriculum_dataset.py",
      "module_purpose": "Implements curriculum learning for translation datasets, gradually increasing difficulty during training",
      "key_classes": [
        {
          "name": "CurriculumTranslationDataset",
          "purpose": "Dataset that implements curriculum learning strategies for translation tasks",
          "key_methods": [
            {
              "name": "_calculate_difficulties",
              "signature": "_calculate_difficulties(self) -> List[float]",
              "brief_description": "Calculate difficulty scores for all examples based on selected strategy"
            },
            {
              "name": "update_stage",
              "signature": "update_stage(self, new_stage: int) -> None",
              "brief_description": "Update the curriculum stage to expose more complex examples"
            },
            {
              "name": "__getitem__",
              "signature": "__getitem__(self, idx: int) -> Dict[str, torch.Tensor]",
              "brief_description": "Get an item from the dataset based on curriculum stage"
            },
            {
              "name": "get_curriculum_stats",
              "signature": "get_curriculum_stats(self) -> Dict[str, Any]",
              "brief_description": "Get statistics about the current curriculum stage"
            }
          ],
          "inheritance": "Dataset",
          "dependencies": [
            "torch.utils.data.Dataset",
            "numpy",
            "collections.Counter"
          ]
        }
      ],
      "key_functions": [],
      "external_dependencies": [
        "torch",
        "numpy",
        "collections"
      ],
      "complexity_score": 7,
      "module_path": "src/data/curriculum_dataset.py"
    },
    {
      "filename": "image_dataset.py",
      "module_purpose": "Provides dataset functionality for loading and preprocessing image data for vision transformer models",
      "key_classes": [
        {
          "name": "ImageDataset",
          "purpose": "Dataset for loading and preprocessing images for vision transformer models",
          "key_methods": [
            {
              "name": "_get_class_idx",
              "signature": "_get_class_idx(self, class_name: str) -> int",
              "brief_description": "Get class index from class name using mapping or dynamic creation"
            },
            {
              "name": "__getitem__",
              "signature": "__getitem__(self, idx: int) -> Dict[str, torch.Tensor]",
              "brief_description": "Load, preprocess and return an image with its label and path"
            }
          ],
          "inheritance": "Dataset",
          "dependencies": [
            "torch.utils.data.Dataset",
            "PIL.Image",
            "ImagePreprocessor"
          ]
        }
      ],
      "key_functions": [],
      "external_dependencies": [
        "torch",
        "PIL",
        "pathlib",
        "json"
      ],
      "complexity_score": 4,
      "module_path": "src/data/image_dataset.py"
    },
    {
      "filename": "wmt_dataloader.py",
      "module_purpose": "Provides a data loader for WMT (Workshop on Machine Translation) parallel corpus",
      "key_classes": [
        {
          "name": "WMTDataLoader",
          "purpose": "Loads and preprocesses WMT parallel corpus data with batching capability",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, data_dir: str, source_lang: str, target_lang: str, batch_size: int = 32, max_examples: Optional[int] = None, seed: int = 42, shuffle: bool = True)",
              "brief_description": "Initialize the WMT data loader with configurable batch size and filtering"
            },
            {
              "name": "load_data",
              "signature": "load_data(self) -> Tuple[List[str], List[str]]",
              "brief_description": "Load and preprocess parallel data"
            },
            {
              "name": "__iter__",
              "signature": "__iter__(self) -> Tuple[List[str], List[str]]",
              "brief_description": "Yield batches of source and target sentences"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "os",
            "random"
          ]
        }
      ],
      "external_dependencies": [
        "os",
        "random"
      ],
      "complexity_score": 3,
      "module_path": "src/data/wmt_dataloader.py"
    },
    {
      "filename": "combined_dataset.py",
      "module_path": "src/data/combined_dataset.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "iwslt_dataset.py",
      "module_purpose": "Provides a dataset class for loading and preprocessing IWSLT dataset for machine translation",
      "key_classes": [
        {
          "name": "IWSLTDataset",
          "purpose": "Handles loading and preprocessing parallel text data from the IWSLT dataset",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, src_lang='de', tgt_lang='en', year='2017', split='train', max_examples=None, data_dir='data/iwslt', random_seed=42, combine_years=True)",
              "brief_description": "Initialize the dataset with source/target languages and processing options"
            },
            {
              "name": "download_data",
              "signature": "download_data(self, year=None)",
              "brief_description": "Download and prepare the IWSLT dataset for a specific year with fallback to synthetic data generation"
            },
            {
              "name": "load_data",
              "signature": "load_data(self) -> Tuple[List[str], List[str]]",
              "brief_description": "Load and preprocess parallel corpora, combining data from multiple years if needed"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "os",
            "random",
            "requests",
            "tqdm",
            "tarfile",
            "io"
          ]
        }
      ],
      "external_dependencies": [
        "datasets"
      ],
      "complexity_score": 5,
      "module_path": "src/data/iwslt_dataset.py"
    },
    {
      "filename": "language_modeling.py",
      "module_purpose": "Implements dataset and dataloaders for language modeling tasks with efficient tokenization and batching",
      "key_classes": [
        {
          "name": "LanguageModelingDataset",
          "purpose": "Dataset for causal language modeling with next-token prediction setup",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, texts: List[str], tokenizer: BPETokenizer, max_length: int = 512, pad_idx: int = 0, bos_idx: int = 1, eos_idx: int = 2)",
              "brief_description": "Initializes dataset and tokenizes all texts upfront for efficiency"
            },
            {
              "name": "__getitem__",
              "signature": "__getitem__(self, idx: int) -> Dict[str, torch.Tensor]",
              "brief_description": "Creates input-target pairs for next-token prediction tasks"
            }
          ],
          "inheritance": "Dataset",
          "dependencies": [
            "torch",
            "torch.utils.data",
            "tokenization.BPETokenizer"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "lm_collate_fn",
          "signature": "lm_collate_fn(batch: List[Dict[str, torch.Tensor]], pad_idx: int) -> Dict[str, torch.Tensor]",
          "brief_description": "Collates and pads batches for efficient training"
        },
        {
          "name": "create_lm_dataloaders",
          "signature": "create_lm_dataloaders(texts: List[str], tokenizer: BPETokenizer, batch_size: int = 16, max_length: int = 512, val_split: float = 0.1, seed: int = 42) -> tuple",
          "brief_description": "Creates training and validation dataloaders with proper data splitting"
        }
      ],
      "external_dependencies": [
        "torch",
        "tqdm",
        "random"
      ],
      "complexity_score": 5,
      "module_path": "src/data/language_modeling.py"
    },
    {
      "filename": "europarl_dataset.py",
      "module_purpose": "Provides a dataset class for loading and preprocessing Europarl parallel corpus data",
      "key_classes": [
        {
          "name": "EuroparlDataset",
          "purpose": "Handles loading and preprocessing parallel text data from the Europarl corpus",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, data_dir: str = 'data/europarl', src_lang: str = 'de', tgt_lang: str = 'en', max_examples: Optional[int] = None, random_seed: int = 42)",
              "brief_description": "Initialize the dataset with language pair and optional filtering"
            },
            {
              "name": "load_data",
              "signature": "load_data(self) -> Tuple[List[str], List[str]]",
              "brief_description": "Load and preprocess parallel data with multiple file pattern detection"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "os",
            "random"
          ]
        }
      ],
      "external_dependencies": [
        "os",
        "random"
      ],
      "complexity_score": 4,
      "module_path": "src/data/europarl_dataset.py"
    },
    {
      "filename": "combined_wmt_translation_dataset.py",
      "module_path": "src/data/combined_wmt_translation_dataset.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "combined_translation_dataset.py",
      "module_purpose": "Implements dataset class for combining multiple translation datasets with configurable sampling",
      "key_classes": [
        {
          "name": "CombinedTranslationDataset",
          "purpose": "Combines samples from multiple translation datasets for unified training",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, src_lang: str = 'de', tgt_lang: str = 'en', datasets: Dict[str, int] = None, seed: int = 42)",
              "brief_description": "Initialize the combined dataset with configurable sources and sample counts"
            }
          ],
          "inheritance": "",
          "dependencies": [
            ".europarl_dataset",
            ".opensubtitles_dataset"
          ]
        }
      ],
      "external_dependencies": [],
      "complexity_score": 2,
      "module_path": "src/data/combined_translation_dataset.py"
    },
    {
      "filename": "multimodal_dataset.py",
      "module_path": "src/data/multimodal_dataset.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "preprocessing.py",
      "module_purpose": "Provides data preprocessing utilities for time series data and machine learning datasets",
      "key_classes": [
        {
          "name": "DataPreprocessor",
          "purpose": "Handles data preprocessing operations like standardization and normalization",
          "key_methods": [
            {
              "name": "fit",
              "signature": "fit(self, data: Union[torch.Tensor, np.ndarray]) -> None",
              "brief_description": "Fit the preprocessor on the data"
            },
            {
              "name": "transform",
              "signature": "transform(self, data: Union[torch.Tensor, np.ndarray]) -> torch.Tensor",
              "brief_description": "Transform the data using the fitted preprocessor"
            },
            {
              "name": "fit_transform",
              "signature": "fit_transform(self, data: Union[torch.Tensor, np.ndarray]) -> torch.Tensor",
              "brief_description": "Fit the preprocessor and transform the data"
            },
            {
              "name": "inverse_transform",
              "signature": "inverse_transform(self, data: Union[torch.Tensor, np.ndarray]) -> torch.Tensor",
              "brief_description": "Inverse transform data back to original scale"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch",
            "numpy",
            "sklearn.preprocessing"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "create_sequences",
          "signature": "create_sequences(data: torch.Tensor, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]",
          "brief_description": "Create input-target sequences from time series data"
        },
        {
          "name": "split_data",
          "signature": "split_data(data: torch.Tensor, train_ratio: float = 0.8, val_ratio: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
          "brief_description": "Split data into train, validation, and test sets"
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy",
        "sklearn"
      ],
      "complexity_score": 4,
      "module_path": "src/data/preprocessing.py"
    },
    {
      "filename": "sequence_data.py",
      "module_purpose": "Provides dataset and dataloader utilities for transformer sequence-to-sequence tasks",
      "key_classes": [
        {
          "name": "TransformerDataset",
          "purpose": "Dataset for handling tokenized source and target sequences for transformer models",
          "key_methods": [
            {
              "name": "__getitem__",
              "signature": "__getitem__(self, idx)",
              "brief_description": "Get source and target sequences with proper BOS/EOS tokens and truncation"
            }
          ],
          "inheritance": "Dataset",
          "dependencies": [
            "torch.utils.data.Dataset"
          ]
        },
        {
          "name": "TransformerCollator",
          "purpose": "Collator class for batching transformer sequences with padding",
          "key_methods": [
            {
              "name": "__call__",
              "signature": "__call__(self, batch: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]",
              "brief_description": "Collate a batch of data with proper padding"
            }
          ],
          "inheritance": "object",
          "dependencies": []
        },
        {
          "name": "TransformerDataModule",
          "purpose": "Complete data module for handling loading, preprocessing, and batching transformer data",
          "key_methods": [
            {
              "name": "_setup",
              "signature": "_setup(self)",
              "brief_description": "Set up datasets and dataloaders with train/validation splits"
            },
            {
              "name": "get_train_dataloader",
              "signature": "get_train_dataloader(self)",
              "brief_description": "Get the training dataloader"
            },
            {
              "name": "get_val_dataloader",
              "signature": "get_val_dataloader(self)",
              "brief_description": "Get the validation dataloader"
            },
            {
              "name": "_collate_fn",
              "signature": "_collate_fn(self, batch)",
              "brief_description": "Collate function to create batches with padding"
            },
            {
              "name": "update_curriculum_stage",
              "signature": "update_curriculum_stage(self, epoch: int) -> None",
              "brief_description": "Update curriculum stage based on epoch"
            },
            {
              "name": "get_curriculum_stats",
              "signature": "get_curriculum_stats(self) -> Dict[str, Any]",
              "brief_description": "Get statistics about the current curriculum stage"
            },
            {
              "name": "estimate_steps_per_epoch",
              "signature": "estimate_steps_per_epoch(self) -> int",
              "brief_description": "Estimate the number of steps per epoch"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "torch.utils.data.DataLoader",
            "numpy"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "transformer_collate_fn",
          "signature": "transformer_collate_fn(batch: List[Dict[str, List[int]]], pad_idx: int) -> Dict[str, torch.Tensor]",
          "brief_description": "Collate function that pads sequences to the same length within a batch"
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy"
      ],
      "complexity_score": 6,
      "module_path": "src/data/sequence_data.py"
    },
    {
      "filename": "opensubtitles_dataset.py",
      "module_purpose": "Provides a dataset class for loading and preprocessing OpenSubtitles parallel corpus data for machine translation",
      "key_classes": [
        {
          "name": "OpenSubtitlesDataset",
          "purpose": "Handles loading and preprocessing parallel text data from the OpenSubtitles corpus",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, data_dir: str = 'data/os', src_lang: str = 'de', tgt_lang: str = 'en', max_examples: Optional[int] = None, random_seed: int = 42)",
              "brief_description": "Initialize the dataset with source/target languages and processing options"
            },
            {
              "name": "load_data",
              "signature": "load_data(self) -> Tuple[List[str], List[str]]",
              "brief_description": "Load and preprocess parallel corpora with support for multiple file patterns"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "os",
            "random",
            "typing"
          ]
        }
      ],
      "external_dependencies": [],
      "complexity_score": 4,
      "module_path": "src/data/opensubtitles_dataset.py"
    },
    {
      "filename": "dataloader.py",
      "module_purpose": "Provides utilities for handling multimodal data with PyTorch DataLoader",
      "key_classes": [
        {
          "name": "MultimodalDataset",
          "purpose": "Dataset class for handling multiple modalities of data with consistent length validation",
          "key_methods": [
            {
              "name": "__getitem__",
              "signature": "__getitem__(self, idx: int) -> Dict[str, torch.Tensor]",
              "brief_description": "Get an item from each modality at the specified index"
            },
            {
              "name": "__len__",
              "signature": "__len__(self) -> int",
              "brief_description": "Return the length of the dataset"
            }
          ],
          "inheritance": "Dataset",
          "dependencies": [
            "torch.utils.data.Dataset"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "create_dataloader",
          "signature": "create_dataloader(dataset: Dataset, batch_size: int = 32, shuffle: bool = True, num_workers: int = 0, pin_memory: bool = True, drop_last: bool = False, collate_fn: Optional[callable] = None) -> DataLoader",
          "brief_description": "Create a DataLoader from a dataset with common configuration options"
        },
        {
          "name": "collate_fn",
          "signature": "collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]",
          "brief_description": "Custom collate function for batching multimodal data"
        },
        {
          "name": "get_dataloaders",
          "signature": "get_dataloaders(train_data: Dict[str, torch.Tensor], val_data: Optional[Dict[str, torch.Tensor]] = None, test_data: Optional[Dict[str, torch.Tensor]] = None, batch_size: int = 32, num_workers: int = 0) -> Tuple[DataLoader, Optional[DataLoader], Optional[DataLoader]]",
          "brief_description": "Create DataLoaders for train, validation, and test sets"
        }
      ],
      "external_dependencies": [
        "torch",
        "numpy"
      ],
      "complexity_score": 3,
      "module_path": "src/data/dataloader.py"
    },
    {
      "filename": "dataset_wrapper.py",
      "module_purpose": "Provides dataset wrappers to standardize interfaces for different dataset types",
      "key_classes": [
        {
          "name": "DictionaryDataset",
          "purpose": "Wrapper for datasets that return tuples, converting them to dictionaries for a standardized interface",
          "key_methods": [
            {
              "name": "__getitem__",
              "signature": "__getitem__(self, idx: int) -> Dict[str, Any]",
              "brief_description": "Get a sample from the dataset as a dictionary with standardized keys"
            }
          ],
          "inheritance": "Dataset",
          "dependencies": [
            "torch.utils.data.Dataset"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "create_dictionary_dataloader",
          "signature": "create_dictionary_dataloader(dataset: Dataset, batch_size: int = 32, shuffle: bool = True, num_workers: int = 4, pin_memory: bool = True, keys: List[str] = None) -> DataLoader",
          "brief_description": "Create a DataLoader that returns dictionary-format batches"
        },
        {
          "name": "load_cifar10_dict",
          "signature": "load_cifar10_dict(batch_size: int = 128, num_workers: int = 4, image_size: int = 32) -> Tuple[DataLoader, DataLoader, List[str]]",
          "brief_description": "Load CIFAR-10 dataset with dictionary-format data loaders"
        }
      ],
      "external_dependencies": [
        "torch",
        "torch.utils.data"
      ],
      "complexity_score": 3,
      "module_path": "src/data/dataset_wrapper.py"
    },
    {
      "filename": "wikipedia_dataset.py",
      "module_purpose": "Provides a dataset class for loading and preprocessing Wikipedia Web2M data from TFRecord format",
      "key_classes": [
        {
          "name": "WikipediaDataset",
          "purpose": "Handles loading and preprocessing multimodal (image-text) data from WikiWeb2M TFRecords",
          "key_methods": [
            {
              "name": "__init__",
              "signature": "__init__(self, data_dir: str = 'data/wiki', split: str = 'train', max_examples: Optional[int] = None, cache_processed_data: bool = True, cache_dir: Optional[str] = None, image_size: int = 224, random_seed: int = 42)",
              "brief_description": "Initialize the dataset with data split and processing options"
            },
            {
              "name": "load_data",
              "signature": "load_data(self) -> Dict[str, List[Any]]",
              "brief_description": "Load and preprocess data from TFRecord files with caching capability"
            },
            {
              "name": "to_pytorch_dataset",
              "signature": "to_pytorch_dataset(self)",
              "brief_description": "Convert to a PyTorch dataset compatible with MultimodalDataset"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "tensorflow",
            "torch",
            "numpy",
            "tqdm"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "create_wiki_dataloaders",
          "signature": "create_wiki_dataloaders(data_dir: str = 'data/wiki', batch_size: int = 32, max_examples: Optional[Dict[str, int]] = None, num_workers: int = 0, image_size: int = 224, random_seed: int = 42) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]",
          "brief_description": "Create DataLoaders for train, validation, and test sets"
        }
      ],
      "external_dependencies": [
        "tensorflow",
        "torch",
        "numpy",
        "tqdm"
      ],
      "complexity_score": 5,
      "module_path": "src/data/wikipedia_dataset.py"
    },
    {
      "filename": "wmt_bpe_tokenizer.py",
      "module_path": "src/data/tokenization/wmt_bpe_tokenizer.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "work_tokenizer.py",
      "module_path": "src/data/tokenization/work_tokenizer.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "optimized_bpe_tokenizer.py",
      "module_purpose": "Implements an optimized Byte Pair Encoding tokenizer with smart caching and batch processing",
      "key_classes": [
        {
          "name": "LRUCache",
          "purpose": "LRU (Least Recently Used) Cache with expiration for efficient tokenizer caching",
          "key_methods": [
            {
              "name": "get",
              "signature": "get(self, key: Any) -> Optional[Any]",
              "brief_description": "Get a value from the cache with expiration checking"
            },
            {
              "name": "put",
              "signature": "put(self, key: Any, value: Any) -> None",
              "brief_description": "Add or update a value in the cache with timestamp"
            },
            {
              "name": "_cleanup_expired",
              "signature": "_cleanup_expired(self) -> None",
              "brief_description": "Remove expired items from cache"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "collections.OrderedDict",
            "threading"
          ]
        },
        {
          "name": "OptimizedBPETokenizer",
          "purpose": "High-performance BPE tokenizer with caching, vectorization, and memory efficiency",
          "key_methods": [
            {
              "name": "tokenize",
              "signature": "tokenize(self, text: str) -> List[str]",
              "brief_description": "Convert text to tokens with caching and validation"
            },
            {
              "name": "encode",
              "signature": "encode(self, text: str) -> List[int]",
              "brief_description": "Convert text to token indices"
            },
            {
              "name": "batch_encode_optimized",
              "signature": "batch_encode_optimized(self, texts: List[str], batch_size: Optional[int] = None) -> List[List[int]]",
              "brief_description": "Encode a batch of texts with optimized processing"
            },
            {
              "name": "train",
              "signature": "train(self, texts: List[str], vocab_size: Optional[int] = None, min_frequency: int = 2, show_progress: bool = True) -> None",
              "brief_description": "Train the BPE tokenizer on a corpus of texts"
            }
          ],
          "inheritance": "BaseTokenizer",
          "dependencies": [
            ".base_tokenizer",
            ".vocabulary",
            "torch",
            "psutil"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "preprocess_data_with_optimized_bpe",
          "signature": "preprocess_data_with_optimized_bpe(dataset, de_tokenizer, en_tokenizer, batch_size=4000, use_multiprocessing=False, num_workers=4)",
          "brief_description": "Efficiently preprocess translation data with optimized BPE tokenizers"
        }
      ],
      "external_dependencies": [
        "torch",
        "psutil",
        "tqdm",
        "threading",
        "json"
      ],
      "complexity_score": 9,
      "module_path": "src/data/tokenization/optimized_bpe_tokenizer.py"
    },
    {
      "filename": "base_tokenizer.py",
      "module_purpose": "Defines the abstract base class for all tokenizers in the system with standard interface",
      "key_classes": [
        {
          "name": "BaseTokenizer",
          "purpose": "Abstract base class that defines the standard interface for all tokenizer implementations",
          "key_methods": [
            {
              "name": "tokenize",
              "signature": "tokenize(self, text: str) -> List[str]",
              "brief_description": "Convert text into tokens"
            },
            {
              "name": "encode",
              "signature": "encode(self, text: str) -> List[int]",
              "brief_description": "Convert text to token indices"
            },
            {
              "name": "decode",
              "signature": "decode(self, token_ids: List[int]) -> str",
              "brief_description": "Convert token indices back to text"
            },
            {
              "name": "batch_encode",
              "signature": "batch_encode(self, texts: List[str]) -> List[List[int]]",
              "brief_description": "Encode multiple texts efficiently"
            },
            {
              "name": "vocab_size",
              "signature": "vocab_size(self) -> int",
              "brief_description": "Get the size of the vocabulary"
            },
            {
              "name": "special_tokens",
              "signature": "special_tokens(self) -> Dict[str, int]",
              "brief_description": "Get the special tokens used by this tokenizer"
            }
          ],
          "inheritance": "ABC",
          "dependencies": [
            "abc.ABC"
          ]
        }
      ],
      "external_dependencies": [
        "abc"
      ],
      "complexity_score": 2,
      "module_path": "src/data/tokenization/base_tokenizer.py"
    },
    {
      "filename": "utils.py",
      "module_path": "src/data/tokenization/utils.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "preprocessing.py",
      "module_purpose": "Provides text preprocessing utilities for tokenization including Unicode normalization and text cleaning",
      "key_classes": [],
      "key_functions": [
        {
          "name": "normalize_unicode",
          "signature": "normalize_unicode(text: str) -> str",
          "brief_description": "Normalize Unicode characters in text using NFKC form"
        },
        {
          "name": "clean_text",
          "signature": "clean_text(text: str, lower: bool = True, remove_accents: bool = False, strip_html: bool = True, handle_contractions: bool = True) -> str",
          "brief_description": "Clean and normalize text with configurable options for case, accents, HTML, and contractions"
        },
        {
          "name": "segment_on_punc",
          "signature": "segment_on_punc(text: str) -> str",
          "brief_description": "Add spaces around punctuation to help with tokenization"
        }
      ],
      "external_dependencies": [
        "re",
        "unicodedata",
        "html"
      ],
      "complexity_score": 3,
      "module_path": "src/data/tokenization/preprocessing.py"
    },
    {
      "filename": "bpe_tokenizer.py",
      "module_purpose": "Implements Byte Pair Encoding tokenizer for subword tokenization with merge operations",
      "key_classes": [
        {
          "name": "BPETokenizer",
          "purpose": "Tokenizer that implements Byte Pair Encoding algorithm for subword tokenization",
          "key_methods": [
            {
              "name": "preprocess",
              "signature": "preprocess(self, text: str) -> str",
              "brief_description": "Preprocess text before tokenization"
            },
            {
              "name": "train",
              "signature": "train(self, texts: List[str], vocab_size: Optional[int] = None, min_frequency: int = 2, show_progress: bool = True) -> None",
              "brief_description": "Train the BPE tokenizer on a corpus of texts by iteratively merging frequent character pairs"
            },
            {
              "name": "_tokenize_word",
              "signature": "_tokenize_word(self, word: str) -> List[str]",
              "brief_description": "Tokenize a single word using BPE merge operations"
            },
            {
              "name": "tokenize",
              "signature": "tokenize(self, text: str) -> List[str]",
              "brief_description": "Convert text into subword tokens based on learned merge operations"
            },
            {
              "name": "encode",
              "signature": "encode(self, text: str) -> List[int]",
              "brief_description": "Convert text to token indices using the vocabulary"
            },
            {
              "name": "batch_encode",
              "signature": "batch_encode(self, texts: List[str]) -> List[List[int]]",
              "brief_description": "Encode a batch of texts into token indices"
            },
            {
              "name": "decode",
              "signature": "decode(self, token_ids: List[int]) -> str",
              "brief_description": "Convert token indices back to text"
            },
            {
              "name": "save_pretrained",
              "signature": "save_pretrained(self, path: str) -> None",
              "brief_description": "Save tokenizer configuration, vocabulary and merges to disk"
            },
            {
              "name": "from_pretrained",
              "signature": "from_pretrained(cls, path: str) -> 'BPETokenizer'",
              "brief_description": "Load a tokenizer from a saved directory"
            },
            {
              "name": "vocab_size",
              "signature": "vocab_size(self) -> int",
              "brief_description": "Get the size of the tokenizer vocabulary (property)"
            },
            {
              "name": "special_tokens",
              "signature": "special_tokens(self) -> Dict[str, int]",
              "brief_description": "Get the special token IDs (property)"
            }
          ],
          "inheritance": "BaseTokenizer",
          "dependencies": [
            ".base_tokenizer",
            ".vocabulary",
            ".preprocessing"
          ]
        }
      ],
      "key_functions": [
        {
          "name": "preprocess_data_with_optimized_bpe",
          "signature": "preprocess_data_with_optimized_bpe(dataset, de_tokenizer, en_tokenizer, batch_size=4000, use_multiprocessing=False, num_workers=4)",
          "brief_description": "Optimized preprocessing function for translation datasets"
        },
        {
          "name": "_preprocess_without_multiprocessing",
          "signature": "_preprocess_without_multiprocessing(dataset, de_tokenizer, en_tokenizer, batch_size)",
          "brief_description": "Process dataset without multiprocessing (better for GPU utilization)"
        },
        {
          "name": "_preprocess_with_multiprocessing",
          "signature": "_preprocess_with_multiprocessing(dataset, de_tokenizer, en_tokenizer, batch_size, num_workers)",
          "brief_description": "Process dataset with multiprocessing (better for CPU-bound tasks)"
        }
      ],
      "external_dependencies": [
        "json",
        "tqdm",
        "collections.Counter",
        "multiprocessing.Pool"
      ],
      "complexity_score": 7,
      "module_path": "src/data/tokenization/bpe_tokenizer.py"
    },
    {
      "filename": "vocabulary.py",
      "module_purpose": "Implements a flexible vocabulary system for mapping between tokens and indices with special token support",
      "key_classes": [
        {
          "name": "Vocabulary",
          "purpose": "Manages token-to-index and index-to-token mappings with special token handling",
          "key_methods": [
            {
              "name": "add_token",
              "signature": "add_token(self, token: str) -> int",
              "brief_description": "Add a token to the vocabulary and return its index"
            },
            {
              "name": "token_to_index",
              "signature": "token_to_index(self, token: str) -> int",
              "brief_description": "Convert a token to its index with validation"
            },
            {
              "name": "index_to_token",
              "signature": "index_to_token(self, idx: int) -> str",
              "brief_description": "Convert an index to its token with validation"
            },
            {
              "name": "tokens_to_indices",
              "signature": "tokens_to_indices(self, tokens: List[str]) -> List[int]",
              "brief_description": "Convert a list of tokens to their indices"
            },
            {
              "name": "indices_to_tokens",
              "signature": "indices_to_tokens(self, indices: List[int]) -> List[str]",
              "brief_description": "Convert a list of indices to their tokens"
            },
            {
              "name": "save",
              "signature": "save(self, path: str) -> None",
              "brief_description": "Save vocabulary to a JSON file"
            },
            {
              "name": "load",
              "signature": "load(cls, path: str) -> 'Vocabulary'",
              "brief_description": "Load vocabulary from a JSON file"
            },
            {
              "name": "build_from_texts",
              "signature": "build_from_texts(cls, texts: List[str], tokenizer, max_vocab_size: Optional[int] = None, min_freq: int = 1, **kwargs) -> 'Vocabulary'",
              "brief_description": "Build a vocabulary from a list of texts using the provided tokenizer"
            }
          ],
          "inheritance": "object",
          "dependencies": [
            "json",
            "collections.Counter",
            "logging"
          ]
        }
      ],
      "external_dependencies": [
        "json",
        "collections.Counter",
        "logging"
      ],
      "complexity_score": 6,
      "module_path": "src/data/tokenization/vocabulary.py"
    },
    {
      "filename": "turbo_bpe_preprocessor.py",
      "module_path": "src/data/tokenization/turbo_bpe_preprocessor.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    },
    {
      "filename": "simple_tokenizer.py",
      "module_path": "src/data/tokenization/simple_tokenizer.py",
      "module_purpose": "No metadata function available",
      "has_metadata_function": false
    }
  ],
  "summary": {
    "total_modules": 78,
    "modules_with_metadata": 65,
    "modules_without_metadata": 13
  },
  "complexity_summary": {
    "average_complexity": 5.538461538461538,
    "modules_by_complexity": [
      {
        "module": "src/utils/profiling.py",
        "score": 9
      },
      {
        "module": "src/models/transformer.py",
        "score": 9
      },
      {
        "module": "src/safety/harness.py",
        "score": 9
      },
      {
        "module": "src/safety/utils.py",
        "score": 9
      },
      {
        "module": "src/data/tokenization/optimized_bpe_tokenizer.py",
        "score": 9
      },
      {
        "module": "src/optimization/quantization.py",
        "score": 8
      },
      {
        "module": "src/training/vision_transformer_trainer.py",
        "score": 8
      },
      {
        "module": "src/training/optimizers.py",
        "score": 8
      },
      {
        "module": "src/training/language_model_trainer.py",
        "score": 8
      },
      {
        "module": "src/training/transformer_trainer.py",
        "score": 8
      },
      {
        "module": "src/models/attention.py",
        "score": 8
      },
      {
        "module": "src/models/vision/vision_transformer.py",
        "score": 8
      },
      {
        "module": "src/safety/integration.py",
        "score": 8
      },
      {
        "module": "src/safety/filter.py",
        "score": 8
      },
      {
        "module": "src/safety/evaluator.py",
        "score": 8
      },
      {
        "module": "src/safety/red_teaming/evaluator.py",
        "score": 8
      },
      {
        "module": "src/optimization/pruning.py",
        "score": 7
      },
      {
        "module": "src/training/metrics.py",
        "score": 7
      },
      {
        "module": "src/models/text_generation.py",
        "score": 7
      },
      {
        "module": "src/models/positional.py",
        "score": 7
      },
      {
        "module": "src/models/vision/multimodal_integration.py",
        "score": 7
      },
      {
        "module": "src/safety/red_teaming/framework.py",
        "score": 7
      },
      {
        "module": "src/safety/red_teaming/generators.py",
        "score": 7
      },
      {
        "module": "src/evaluation/language_model_evaluation.py",
        "score": 7
      },
      {
        "module": "src/data/curriculum_dataset.py",
        "score": 7
      },
      {
        "module": "src/data/tokenization/bpe_tokenizer.py",
        "score": 7
      },
      {
        "module": "src/optimization/benchmarking.py",
        "score": 6
      },
      {
        "module": "src/training/transformer_utils.py",
        "score": 6
      },
      {
        "module": "src/training/trainer.py",
        "score": 6
      },
      {
        "module": "src/models/feed_forward.py",
        "score": 6
      },
      {
        "module": "src/models/vision/image_preprocessing.py",
        "score": 6
      },
      {
        "module": "src/data/sequence_data.py",
        "score": 6
      },
      {
        "module": "src/data/tokenization/vocabulary.py",
        "score": 6
      },
      {
        "module": "src/optimization/mixed_precision.py",
        "score": 5
      },
      {
        "module": "src/utils/visualization.py",
        "score": 5
      },
      {
        "module": "src/models/vision/patch_embedding.py",
        "score": 5
      },
      {
        "module": "src/models/pretrained/clip_model.py",
        "score": 5
      },
      {
        "module": "src/data/iwslt_dataset.py",
        "score": 5
      },
      {
        "module": "src/data/language_modeling.py",
        "score": 5
      },
      {
        "module": "src/data/wikipedia_dataset.py",
        "score": 5
      },
      {
        "module": "src/training/losses.py",
        "score": 4
      },
      {
        "module": "src/utils/logging.py",
        "score": 4
      },
      {
        "module": "src/utils/list_models.py",
        "score": 4
      },
      {
        "module": "src/models/layers.py",
        "score": 4
      },
      {
        "module": "src/models/pretrained/base_wrapper.py",
        "score": 4
      },
      {
        "module": "src/data/wmt_dataset.py",
        "score": 4
      },
      {
        "module": "src/data/image_dataset.py",
        "score": 4
      },
      {
        "module": "src/data/europarl_dataset.py",
        "score": 4
      },
      {
        "module": "src/data/preprocessing.py",
        "score": 4
      },
      {
        "module": "src/data/opensubtitles_dataset.py",
        "score": 4
      },
      {
        "module": "src/training/joint_bpe_training.py",
        "score": 3
      },
      {
        "module": "src/utils/config.py",
        "score": 3
      },
      {
        "module": "src/models/base_model.py",
        "score": 3
      },
      {
        "module": "src/models/pretrained/vision_transformer.py",
        "score": 3
      },
      {
        "module": "src/models/pretrained/model_registry.py",
        "score": 3
      },
      {
        "module": "src/models/pretrained/adapters.py",
        "score": 3
      },
      {
        "module": "src/evaluation/translation_metrics.py",
        "score": 3
      },
      {
        "module": "src/data/wmt_dataloader.py",
        "score": 3
      },
      {
        "module": "src/data/dataloader.py",
        "score": 3
      },
      {
        "module": "src/data/dataset_wrapper.py",
        "score": 3
      },
      {
        "module": "src/data/tokenization/preprocessing.py",
        "score": 3
      },
      {
        "module": "src/models/embeddings.py",
        "score": 2
      },
      {
        "module": "src/data/combined_translation_dataset.py",
        "score": 2
      },
      {
        "module": "src/data/tokenization/base_tokenizer.py",
        "score": 2
      },
      {
        "module": "src/models/activations.py",
        "score": 1
      }
    ]
  },
  "dependency_summary": {
    "total_external_dependencies": 36,
    "dependencies_by_usage": [
      {
        "name": "torch",
        "usage_count": 43
      },
      {
        "name": "numpy",
        "usage_count": 19
      },
      {
        "name": "json",
        "usage_count": 8
      },
      {
        "name": "matplotlib",
        "usage_count": 8
      },
      {
        "name": "tqdm",
        "usage_count": 8
      },
      {
        "name": "logging",
        "usage_count": 5
      },
      {
        "name": "typing",
        "usage_count": 4
      },
      {
        "name": "re",
        "usage_count": 4
      },
      {
        "name": "random",
        "usage_count": 4
      },
      {
        "name": "os",
        "usage_count": 3
      },
      {
        "name": "seaborn",
        "usage_count": 3
      },
      {
        "name": "nltk",
        "usage_count": 2
      },
      {
        "name": "sklearn",
        "usage_count": 2
      },
      {
        "name": "psutil",
        "usage_count": 2
      },
      {
        "name": "PIL",
        "usage_count": 2
      },
      {
        "name": "datasets",
        "usage_count": 2
      },
      {
        "name": "collections.Counter",
        "usage_count": 2
      },
      {
        "name": "time",
        "usage_count": 1
      },
      {
        "name": "src.data.tokenization",
        "usage_count": 1
      },
      {
        "name": "argparse",
        "usage_count": 1
      },
      {
        "name": "huggingface_hub",
        "usage_count": 1
      },
      {
        "name": "pandas",
        "usage_count": 1
      },
      {
        "name": "torchvision",
        "usage_count": 1
      },
      {
        "name": "math",
        "usage_count": 1
      },
      {
        "name": "transformers",
        "usage_count": 1
      },
      {
        "name": "open_clip",
        "usage_count": 1
      },
      {
        "name": "datetime",
        "usage_count": 1
      },
      {
        "name": "collections",
        "usage_count": 1
      },
      {
        "name": "pathlib",
        "usage_count": 1
      },
      {
        "name": "torch.utils.data",
        "usage_count": 1
      },
      {
        "name": "tensorflow",
        "usage_count": 1
      },
      {
        "name": "threading",
        "usage_count": 1
      },
      {
        "name": "abc",
        "usage_count": 1
      },
      {
        "name": "unicodedata",
        "usage_count": 1
      },
      {
        "name": "html",
        "usage_count": 1
      },
      {
        "name": "multiprocessing.Pool",
        "usage_count": 1
      }
    ]
  }
}