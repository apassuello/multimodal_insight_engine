# multimodal_training_demo.py
"""
Multimodal Integration Training Demo

This script demonstrates the complete training and evaluation pipeline for
multimodal models with contrastive learning. It showcases:

1. Loading and preprocessing multimodal data
2. Building a cross-modal attention transformer
3. Training with advanced contrastive learning objectives
4. Evaluating cross-modal retrieval performance
5. Visualizing model outputs and attention maps

Available contrastive loss modes:
- contrastive: Standard contrastive loss with enhanced configuration
- memory_queue: Uses a memory queue for consistent global comparisons
- dynamic_temp: Automatically adjusts temperature based on embeddings
- hard_negative: Focuses training on challenging negative examples
- mixed: Combines multiple objectives for robust learning

Example usage:
python multimodal_training_demo.py --loss_type memory_queue --queue_size 8192
python multimodal_training_demo.py --loss_type dynamic_temp
python multimodal_training_demo.py --loss_type hard_negative --mining_strategy semi-hard
"""

import os
import torch
import numpy as np
import logging
import random
import math
from datetime import datetime
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("multimodal_training.log")],
)
logger = logging.getLogger(__name__)

# Import our models and utilities
from src.models.vision.image_preprocessing import ImagePreprocessor
from src.data.tokenization.simple_tokenizer import SimpleTokenizer
from src.training.multimodal_trainer import MultimodalTrainer
from src.utils.argument_configs import get_multimodal_training_args
from src.utils.model_utils import (
    print_model_summary,
    convert_tensors_to_python_types,
    count_parameters,
)
from src.models.model_factory import create_multimodal_model
from src.data.multimodal_data_utils import create_data_loaders
from src.training.loss_factory import create_loss_function
from src.evaluation.inference_demo import run_inference_demo


# Add these classes after imports in multimodal_training_demo.py

import torch
import torch.nn as nn
import torch.nn.functional as F


# Enhanced multimodal model with anti-collapse mechanisms
class SimpleMultimodalModel(nn.Module):
    def __init__(self, vision_model, text_model, projection_dim=512):
        super().__init__()
        self.vision_model = vision_model
        self.text_model = text_model

        # Get dimensions
        if hasattr(vision_model, "num_features"):
            vision_dim = vision_model.num_features
        elif hasattr(vision_model, "embed_dim"):
            vision_dim = vision_model.embed_dim
        else:
            vision_dim = 768  # Default

        if hasattr(text_model, "encoder") and hasattr(text_model.encoder, "config"):
            text_dim = text_model.encoder.config.hidden_size
        elif hasattr(text_model, "d_model"):
            text_dim = text_model.d_model
        else:
            text_dim = 768  # Default

        print(
            f"Creating SimpleMultimodalModel with dims: vision={vision_dim}, text={text_dim}, proj={projection_dim}"
        )

        # IMPORTANT CHANGE: Simpler model with explicit normalization layer to prevent collapse
        # Vision projection: single layer with batch norm to enforce feature distribution
        self.vision_proj = nn.Sequential(
            nn.Linear(vision_dim, projection_dim),
            nn.BatchNorm1d(projection_dim, affine=True),  # BN prevents feature collapse
        )

        # Text projection: different structure to ensure asymmetry
        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, projection_dim),
            nn.BatchNorm1d(projection_dim, affine=True),  # BN prevents feature collapse
        )

        # Initialize with strong orthogonal initialization
        # This creates diverse starting features that aren't correlated
        for m in self.vision_proj.modules():
            if isinstance(m, nn.Linear):
                # Orthogonal initialization ensures diverse features from the start
                nn.init.orthogonal_(m.weight, gain=1.4)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.2)

        for m in self.text_proj.modules():
            if isinstance(m, nn.Linear):
                # Different gain for text model to create asymmetry
                nn.init.orthogonal_(m.weight, gain=1.2)
                if m.bias is not None:
                    nn.init.constant_(m.bias, -0.2)

        # BatchNorm params with different initialization to ensure asymmetry
        for name, m in self.named_modules():
            if isinstance(m, nn.BatchNorm1d):
                if "vision" in name:
                    # Vision BN - slightly different from text BN
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.1)
                else:
                    # Text BN - different initialization
                    nn.init.constant_(m.weight, 0.9)
                    nn.init.constant_(m.bias, -0.1)

        # Learnable temperature parameter for similarity scaling
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

        # Track variance statistics to detect collapse
        self.register_buffer("running_vision_var", torch.ones(1))
        self.register_buffer("running_text_var", torch.ones(1))
        self.register_buffer("update_count", torch.zeros(1))

        # Feature whitening/scaling - explicitly prevents feature collapse
        # by rescaling feature distributions
        self.feature_scale = 10.0  # Increased scaling factor to prevent collapse

        print(
            f"Model initialized with anti-collapse techniques: BatchNorm + orthogonal init + feature scaling"
        )
        print(f"Feature scale: {self.feature_scale}")

    def forward(self, images=None, text_data=None, return_attention=False):
        outputs = {}

        # Vision forward
        if images is not None:
            # Extract vision features
            if hasattr(self.vision_model, "forward_features"):
                vision_features = self.vision_model.forward_features(images)
            else:
                vision_features = self.vision_model(images)
            # Handle different output formats
            if isinstance(vision_features, dict) and "pooler_output" in vision_features:
                vision_features = vision_features["pooler_output"]
            elif isinstance(vision_features, tuple):
                vision_features = vision_features[
                    0
                ]  # Typically the features are the first element

            # Store full sequence features for reference
            outputs["vision_features_seq"] = vision_features

            # Get pooled representation if needed
            if len(vision_features.shape) == 3:  # [batch, seq_len, dim]
                # Use CLS token if available (typically first token)
                pooled_vision = vision_features[:, 0]
            else:
                pooled_vision = vision_features

            # Apply projection with BatchNorm to prevent feature collapse
            vision_features = self.vision_proj(pooled_vision)

            # Explicit feature scaling to prevent collapse - higher scale forces features apart
            # This is a critical change - it ensures features don't collapse to the same values
            # by explicitly increasing their scale/variance
            if self.training:
                # Scale features to have specific variance (prevents collapse but don't scale too aggressively)
                vision_features = vision_features * self.feature_scale

            # Track variance statistics
            if self.training:
                with torch.no_grad():
                    batch_var = torch.var(vision_features, dim=0).mean().item()
                    # Update running statistics with momentum
                    self.running_vision_var = (
                        0.9 * self.running_vision_var + 0.1 * batch_var
                    )

            outputs["vision_features"] = vision_features
            outputs["vision_features_enhanced"] = vision_features  # For compatibility

        # Text forward
        if text_data is not None:
            # Extract text features
            if hasattr(self.text_model, "encode"):
                text_features = self.text_model.encode(**text_data)
            else:
                text_features = self.text_model(**text_data)

            # Handle different output formats
            if isinstance(text_features, dict) and "pooler_output" in text_features:
                text_features = text_features["pooler_output"]
            elif isinstance(text_features, tuple):
                text_features = text_features[
                    0
                ]  # Typically the features are the first element

            # Store full sequence features for reference
            outputs["text_features_seq"] = text_features

            # Get pooled representation if needed
            if len(text_features.shape) == 3:  # [batch, seq_len, dim]
                # Use CLS token if available (typically first token)
                pooled_text = text_features[:, 0]
            else:
                pooled_text = text_features

            # Apply projection with BatchNorm to prevent feature collapse
            text_features = self.text_proj(pooled_text)

            # Apply scaling with slightly different factor for asymmetry
            if self.training:
                # Scale features to have specific variance (prevents collapse)
                # Use a slightly different scale from vision to create asymmetry
                text_features = text_features * (
                    self.feature_scale * 0.9
                )  # Increased asymmetry between modalities

            # Track variance statistics
            if self.training:
                with torch.no_grad():
                    batch_var = torch.var(text_features, dim=0).mean().item()
                    # Update running statistics with momentum
                    self.running_text_var = (
                        0.9 * self.running_text_var + 0.1 * batch_var
                    )
                    # Update counter (used for logging)
                    self.update_count += 1

            outputs["text_features"] = text_features
            outputs["text_features_enhanced"] = text_features  # For compatibility

        # If both modalities present, compute similarity
        if images is not None and text_data is not None:
            # Normalize features AFTER scaling to ensure proper normalization
            # This is critical - the normalization removes the scale but maintains the learned directions
            vision_norm = F.normalize(vision_features, p=2, dim=1)
            text_norm = F.normalize(text_features, p=2, dim=1)

            # Compute feature statistics before and after normalization to detect collapse
            if self.training:
                with torch.no_grad():
                    # Calculate pre-normalization variance by feature dimension
                    v_var_by_dim = torch.var(vision_features, dim=0)
                    t_var_by_dim = torch.var(text_features, dim=0)

                    # Calculate post-normalization variance by feature dimension
                    v_norm_var_by_dim = torch.var(vision_norm, dim=0)
                    t_norm_var_by_dim = torch.var(text_norm, dim=0)

                    # Count low-variance dimensions (indication of partial collapse)
                    v_low_var_dims = (v_norm_var_by_dim < 0.01).sum().item()
                    t_low_var_dims = (t_norm_var_by_dim < 0.01).sum().item()

                    # Log warning if many dimensions have low variance
                    if v_low_var_dims > 10 or t_low_var_dims > 10:
                        print(
                            f"WARNING: Many low-variance dimensions - Vision: {v_low_var_dims}, Text: {t_low_var_dims}"
                        )

            # Add powerful regularization loss to prevent feature collapse - force features to be decorrelated
            if self.training:
                # Compute centered features
                vision_centered = vision_norm - vision_norm.mean(dim=0, keepdim=True)
                text_centered = text_norm - text_norm.mean(dim=0, keepdim=True)

                batch_size = vision_norm.size(0)

                # Compute covariance matrices
                vision_cov = torch.matmul(vision_centered.T, vision_centered) / (
                    batch_size - 1
                )
                text_cov = torch.matmul(text_centered.T, text_centered) / (
                    batch_size - 1
                )

                # Identity matrix for computing off-diagonal elements
                I = torch.eye(vision_cov.size(0), device=vision_cov.device)

                # Compute orthogonality loss (stronger regularization than before)
                # This loss specifically targets the off-diagonal elements of the covariance matrix
                # making features more orthogonal/decorrelated
                vision_ortho_loss = torch.sum(torch.pow(vision_cov * (1 - I), 2))
                text_ortho_loss = torch.sum(torch.pow(text_cov * (1 - I), 2))

                # Combined loss with higher weight for strong regularization
                decor_loss = vision_ortho_loss + text_ortho_loss
                outputs["decor_loss"] = (
                    decor_loss * 0.5
                )  # Increased weight for stronger anti-collapse effect

            # Calculate temperature-scaled similarity
            # Clamp logit_scale to prevent extreme values
            logit_scale_value = torch.clamp(self.logit_scale.exp(), min=1.0, max=100.0)
            similarity = logit_scale_value * torch.matmul(vision_norm, text_norm.T)

            outputs["similarity"] = similarity
            outputs["raw_similarity"] = torch.matmul(
                vision_norm, text_norm.T
            )  # Without scaling
            outputs["logit_scale"] = logit_scale_value.item()  # For debugging

            # Track feature statistics for monitoring
            outputs["vision_var"] = self.running_vision_var.item()
            outputs["text_var"] = self.running_text_var.item()

            # Add fusion features (mean of two modalities) for compatibility
            fusion_features = (vision_features + text_features) / 2
            outputs["fusion_features"] = fusion_features
            outputs["pooled_fusion"] = fusion_features

            # Print diagnostics periodically during training
            if self.training and self.update_count % 5 == 0:
                print(
                    f"FEATURE STATS: Vision var: {self.running_vision_var.item():.4f}, Text var: {self.running_text_var.item():.4f}, Scale: {self.feature_scale:.1f}"
                )

        return outputs


# Enhanced supervised contrastive loss with anti-collapse mechanisms
class SupervisedAlignmentLoss(nn.Module):
    def __init__(self, temperature=0.07):  # Use lower temperature for sharper contrasts
        super().__init__()
        self.temperature = (
            temperature  # Lower temperature makes positive pairs more distinct
        )
        self.mse = nn.MSELoss()
        self.dim = 768  # Match model's projection_dim for 768-dim models

        # Add a diversity loss weight to actively prevent feature collapse
        self.diversity_weight = 1.0  # Increased from 0.5 for stronger anti-collapse

        # Track iteration count
        self.iteration = 0

        print(f"Initialized SupervisedAlignmentLoss with temperature={temperature}")

    def forward(self, vision_features, text_features, match_ids=None, **kwargs):
        self.iteration += 1

        # Handle additional losses from model output
        decor_loss = kwargs.get("decor_loss", 0.0)
        extra_loss = 0.0

        if isinstance(decor_loss, torch.Tensor):
            extra_loss = extra_loss + decor_loss

        # Extract logit scale if provided
        logit_scale = kwargs.get("logit_scale", 1.0)
        if not isinstance(logit_scale, float):
            logit_scale = 1.0

        # Always add diversity loss to prevent feature collapse proactively
        # This is the core fix - actively enforce feature diversity in each batch
        if isinstance(vision_features, torch.Tensor) and isinstance(
            text_features, torch.Tensor
        ):
            # Get unnormalized features for better diversity calculation
            batch_size = vision_features.shape[0]

            # Calculate feature statistics to detect collapse
            v_var = torch.var(vision_features, dim=0).mean()
            t_var = torch.var(text_features, dim=0).mean()

            # Log warning if variance is too low (features are collapsing)
            if v_var < 0.01 or t_var < 0.01:
                print(
                    f"WARNING: Feature collapse detected - Vision var: {v_var.item():.6f}, Text var: {t_var.item():.6f}"
                )

            # Calculate similarity matrices
            # For normalizing later, store original norms
            v_norms = torch.norm(vision_features, p=2, dim=1, keepdim=True)
            t_norms = torch.norm(text_features, p=2, dim=1, keepdim=True)

            # Calculate pairwise feature similarity (to reduce, promoting diversity)
            vision_sim = torch.matmul(vision_features, vision_features.T)
            text_sim = torch.matmul(text_features, text_features.T)

            # Identity matrix to mask out diagonal elements (same feature with itself)
            I = torch.eye(batch_size, device=vision_features.device)

            # Diversity loss: minimize similarity between different samples' features
            # This directly prevents feature collapse by making features more orthogonal
            feature_diversity_loss = torch.pow(vision_sim * (1 - I), 2).sum() / (
                batch_size * (batch_size - 1)
            ) + torch.pow(text_sim * (1 - I), 2).sum() / (batch_size * (batch_size - 1))

            # Add to extra loss - stronger weight when variance is low
            diversity_weight = self.diversity_weight * (
                0.1 + 10.0 * torch.exp(-5.0 * (v_var + t_var))
            )
            extra_loss = extra_loss + feature_diversity_loss * diversity_weight

        # Normalize features
        vision_features = F.normalize(vision_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)

        # Compute similarity
        batch_size = vision_features.shape[0]

        # Use adaptive temperature
        effective_temp = self.temperature
        if "logit_scale" in kwargs:
            # If model provides logit scale, use it for consistency
            effective_temp = 1.0 / logit_scale

        sim = torch.matmul(vision_features, text_features.T) / effective_temp

        if match_ids is None:
            # If no match_ids, use diagonal matching (fallback)
            print("WARNING: No match_ids provided, using diagonal matching")
            targets_v2t = torch.arange(batch_size, device=sim.device)
            targets_t2v = torch.arange(batch_size, device=sim.device)
            alignment_loss = 0
        else:
            # Create targets based on match_ids
            targets_v2t = []
            targets_t2v = []

            # For all pairs tracking
            all_matches = {}
            for i in range(batch_size):
                # Group all matches by match_id for efficient lookup
                mid_str = str(match_ids[i])
                if mid_str not in all_matches:
                    all_matches[mid_str] = []
                all_matches[mid_str].append(i)

            # For each item, find matching targets from the same group
            for i in range(batch_size):
                mid_str = str(match_ids[i])
                matches = [j for j in all_matches[mid_str] if j != i]

                if matches:
                    # Randomly select one match
                    match_idx = matches[torch.randint(0, len(matches), (1,)).item()]
                    targets_v2t.append(match_idx)
                else:
                    # If no matches, use itself as target
                    targets_v2t.append(i)

            # Repeat for text to vision direction
            for i in range(batch_size):
                mid_str = str(match_ids[i])
                matches = [j for j in all_matches[mid_str] if j != i]

                if matches:
                    match_idx = matches[torch.randint(0, len(matches), (1,)).item()]
                    targets_t2v.append(match_idx)
                else:
                    targets_t2v.append(i)

            targets_v2t = torch.tensor(targets_v2t, device=sim.device)
            targets_t2v = torch.tensor(targets_t2v, device=sim.device)

            # Compute direct alignment loss within groups with stronger weighting
            alignment_loss = 0
            groups = {}
            for i, mid in enumerate(match_ids):
                mid_str = str(mid)  # Convert to string to ensure consistent keys
                if mid_str not in groups:
                    groups[mid_str] = []
                groups[mid_str].append(i)

            # Track number of valid alignment pairs for normalization
            total_alignment_pairs = 0

            for group_indices in groups.values():
                if len(group_indices) < 2:
                    continue

                # Get features for this group
                group_vision = vision_features[group_indices]
                group_text = text_features[group_indices]

                # Direct pairwise alignment within group with stronger supervision
                for i in range(len(group_indices)):
                    for j in range(i + 1, len(group_indices)):
                        vision_i = group_vision[i]
                        vision_j = group_vision[j]
                        text_i = group_text[i]
                        text_j = group_text[j]

                        # Cross-modal alignment: align vision with matching text (directly)
                        vision_to_text_alignment = F.mse_loss(
                            vision_i, text_j
                        ) + F.mse_loss(vision_j, text_i)

                        # Same-modal cohesion: make same-modality features similar
                        vision_cohesion = F.mse_loss(vision_i, vision_j)
                        text_cohesion = F.mse_loss(text_i, text_j)

                        # Stronger weighting for cross-modal alignment
                        alignment_loss += vision_to_text_alignment + 0.5 * (
                            vision_cohesion + text_cohesion
                        )
                        total_alignment_pairs += 1

                # Compute centroids and align them as well - useful for multi-sample groups
                if len(group_indices) > 2:
                    vision_centroid = group_vision.mean(dim=0)
                    text_centroid = group_text.mean(dim=0)

                    # Centroid alignment is very effective for handling variance across a semantic group
                    centroid_alignment = F.mse_loss(vision_centroid, text_centroid)
                    alignment_loss += centroid_alignment * len(group_indices) * 0.5
                    total_alignment_pairs += 1

            # Normalize alignment loss by number of pairs to keep scale consistent
            if total_alignment_pairs > 0:
                alignment_loss = alignment_loss / total_alignment_pairs

                # Add explicit tracking of positive similarities
                positive_similarities = []
                negative_similarities = []

                # Calculate average similarity for all matching and non-matching pairs
                with torch.no_grad():
                    for i in range(batch_size):
                        for j in range(batch_size):
                            similarity_val = F.cosine_similarity(
                                vision_features[i].unsqueeze(0),
                                text_features[j].unsqueeze(0),
                            ).item()
                            if match_ids[i] == match_ids[j] and i != j:
                                positive_similarities.append(similarity_val)
                            elif match_ids[i] != match_ids[j]:
                                negative_similarities.append(similarity_val)

                # Log the separation stats if we have both positives and negatives
                if positive_similarities and negative_similarities:
                    avg_pos_sim = sum(positive_similarities) / len(
                        positive_similarities
                    )
                    avg_neg_sim = sum(negative_similarities) / len(
                        negative_similarities
                    )
                    separation = avg_pos_sim - avg_neg_sim

                    if self.iteration % 10 == 0:  # Only log occasionally to avoid spam
                        print(
                            f"Similarity stats - Pos: {avg_pos_sim:.4f}, Neg: {avg_neg_sim:.4f}, Gap: {separation:.4f}"
                        )

            # Add hard negative mining for more challenging learning
            # Focus on highly confusing negative pairs (textually similar but semantically different)
            if batch_size > 4 and self.iteration > 10:
                # Find hard negatives - high similarity pairs that shouldn't match
                hard_negatives_loss = 0.0
                hard_count = 0

                # Create match matrix for easy lookup
                match_matrix = torch.zeros(
                    (batch_size, batch_size), dtype=torch.bool, device=sim.device
                )
                for i in range(batch_size):
                    for j in range(batch_size):
                        match_matrix[i, j] = match_ids[i] == match_ids[j]

                # Find top k% most similar non-matching pairs
                with torch.no_grad():
                    similarity = torch.matmul(vision_features, text_features.T)

                    # Create negative mask (where pairs don't match)
                    negative_mask = ~match_matrix

                    # Find hardest negatives (highest similarity non-matches)
                    hard_negative_values, _ = torch.topk(
                        similarity[negative_mask], k=max(5, batch_size // 4)
                    )
                    hard_threshold = hard_negative_values.min()

                    # Create mask for hard negatives
                    hard_negative_mask = (similarity > hard_threshold) & negative_mask

                    # Count hard negatives
                    hard_count = hard_negative_mask.sum().item()

                # Push hard negatives further apart
                if hard_count > 0:
                    # Push apart vision-text pairs that are similar but shouldn't be
                    repulsion_loss = (
                        torch.sum(similarity * hard_negative_mask) / hard_count
                    )
                    hard_negatives_loss = (
                        repulsion_loss * 0.5
                    )  # Lower weight than main loss

                    # Add to alignment loss
                    alignment_loss = alignment_loss + hard_negatives_loss

        # Compute cross-entropy loss for both directions
        loss_v2t = F.cross_entropy(sim, targets_v2t)
        loss_t2v = F.cross_entropy(sim.T, targets_t2v)
        ce_loss = (loss_v2t + loss_t2v) / 2

        # Add contrastive margin to ensure clear separation between positives and negatives
        if batch_size > 4 and match_ids is not None:
            # Create match matrix for easier indexing
            match_matrix = torch.zeros(
                (batch_size, batch_size), dtype=torch.bool, device=sim.device
            )
            for i in range(batch_size):
                for j in range(batch_size):
                    match_matrix[i, j] = match_ids[i] == match_ids[j]

            # Extract diagonal elements (each item with itself)
            diag_mask = torch.eye(batch_size, dtype=torch.bool, device=sim.device)
            match_matrix = match_matrix & ~diag_mask  # Remove self-matches

            # Calculate margin contrastive loss
            pos_margin = 0.8  # Positives should be above this
            neg_margin = 0.3  # Negatives should be below this

            # Margin losses
            pos_mask = match_matrix.float()
            neg_mask = (~match_matrix & ~diag_mask).float()

            # Positive pairs should have high similarity
            pos_loss = torch.sum(F.relu(pos_margin - sim) * pos_mask) / max(
                1, pos_mask.sum()
            )

            # Negative pairs should have lower similarity
            neg_loss = torch.sum(F.relu(sim - neg_margin) * neg_mask) / max(
                1, neg_mask.sum()
            )

            # Add margin loss with appropriate weighting
            margin_loss = (pos_loss + neg_loss) * 0.3
            ce_loss = ce_loss + margin_loss

        # Combine losses with higher alignment weight and include extra losses
        # total_loss = ce_loss + 0.8 * alignment_loss + extra_loss

        # Make sure to log the extra loss components
        if isinstance(extra_loss, torch.Tensor) and extra_loss > 0:
            print(
                f"Extra diversity loss: {extra_loss.item():.4f} with weight {diversity_weight.item():.4f}"
            )

        # Adjust weights to balance contrastive learning with alignment
        total_loss = 0.3 * ce_loss + 1.0 * alignment_loss + extra_loss

        # Compute accuracy
        with torch.no_grad():
            v2t_pred = torch.argmax(sim, dim=1)
            t2v_pred = torch.argmax(sim.T, dim=1)
            v2t_correct = (v2t_pred == targets_v2t).float().mean()
            t2v_correct = (t2v_pred == targets_t2v).float().mean()
            accuracy = (v2t_correct + t2v_correct) / 2

            # Calculate positive-negative separation for diagnostics
            if match_ids is not None and batch_size > 2:
                # Create match matrix
                match_matrix = torch.zeros(
                    (batch_size, batch_size), dtype=torch.bool, device=sim.device
                )
                for i in range(batch_size):
                    for j in range(batch_size):
                        match_matrix[i, j] = match_ids[i] == match_ids[j]

                # Calculate mean similarity for matching vs non-matching pairs
                diag_mask = torch.eye(batch_size, dtype=torch.bool, device=sim.device)
                matches = match_matrix & ~diag_mask  # Remove self-matches
                non_matches = ~match_matrix & ~diag_mask  # Remove self-non-matches

                raw_sim = torch.matmul(vision_features, text_features.T)

                if matches.sum() > 0:
                    pos_sim_mean = raw_sim[matches].mean().item()
                else:
                    pos_sim_mean = 0.0

                if non_matches.sum() > 0:
                    neg_sim_mean = raw_sim[non_matches].mean().item()
                else:
                    neg_sim_mean = 0.0

                # Calculate positive-negative separation
                separation = pos_sim_mean - neg_sim_mean

        # Return comprehensive metrics
        return {
            "loss": total_loss,
            "ce_loss": ce_loss.item(),
            "alignment_loss": (
                alignment_loss.item()
                if isinstance(alignment_loss, torch.Tensor)
                else alignment_loss
            ),
            "v2t_loss": loss_v2t.item(),
            "t2v_loss": loss_t2v.item(),
            "accuracy": accuracy.item(),
            "temperature": effective_temp,
            "pos_sim": pos_sim_mean if "pos_sim_mean" in locals() else 0.0,
            "neg_sim": neg_sim_mean if "neg_sim_mean" in locals() else 0.0,
            "separation": separation if "separation" in locals() else 0.0,
            "extra_loss": (
                extra_loss.item() if isinstance(extra_loss, torch.Tensor) else 0.0
            ),
        }


def set_seed(seed):
    """Set random seed for reproducibility"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    random.seed(seed)


def main():
    """Main function for multimodal training demo."""
    # Parse arguments
    parser = get_multimodal_training_args()
    args = parser.parse_args()
    logging.basicConfig(level=logging.INFO)
    # Set random seed
    set_seed(args.seed)

    # Create output directories
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, args.checkpoint_dir), exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, args.log_dir), exist_ok=True)

    # Set device
    if args.device is None:
        if torch.backends.mps.is_available():
            device = torch.device("mps")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(args.device)

    print(f"Using device: {device}")

    # Create preprocessor and tokenizer
    image_preprocessor = ImagePreprocessor(
        image_size=224,
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    )

    # Create a tokenizer that's compatible with our model choices
    if args.use_pretrained_text:
        # If using dimension-matched presets, use proper tokenizers
        if args.model_size == "small":
            # MiniLM for small preset
            tokenizer = SimpleTokenizer(
                pretrained_model_name="microsoft/MiniLM-L12-H384-uncased",
                max_length=args.max_text_length,
            )
            print(f"Using MiniLM tokenizer with max length {args.max_text_length}")
        elif args.model_size == "medium":
            # FlauBERT for medium preset
            tokenizer = SimpleTokenizer(
                pretrained_model_name="flaubert-small-cased",
                max_length=args.max_text_length,
            )
            print(f"Using FlauBERT tokenizer with max length {args.max_text_length}")
        elif args.model_size == "large":
            # Different tokenizers based on device for large preset
            if (
                torch.backends.mps.is_available()
                and torch.device(args.device).type == "mps"
            ):
                # ALBERT for MPS with large preset
                tokenizer = SimpleTokenizer(
                    pretrained_model_name="albert-base-v2",
                    max_length=args.max_text_length,
                )
                print(f"Using ALBERT tokenizer with max length {args.max_text_length}")
            else:
                # BERT for other devices with large preset
                tokenizer = SimpleTokenizer(
                    pretrained_model_name="bert-base-uncased",
                    max_length=args.max_text_length,
                )
                print(f"Using BERT tokenizer with max length {args.max_text_length}")
        # If using direct HuggingFace model names
        elif (
            "/" in args.text_model
            or args.text_model.startswith("microsoft")
            or args.text_model.startswith("google")
        ):
            # Use the exact model name for the tokenizer
            tokenizer = SimpleTokenizer(
                pretrained_model_name=args.text_model,
                max_length=args.max_text_length,
            )
            print(
                f"Using tokenizer matched to {args.text_model} with max length {args.max_text_length}"
            )
        # Handle standard model names
        elif args.text_model == "mobilebert":
            # Use MobileBERT tokenizer for MobileBERT model
            tokenizer = SimpleTokenizer(
                pretrained_model_name="google/mobilebert-uncased",
                max_length=args.max_text_length,
            )
            print(f"Using MobileBERT tokenizer with max length {args.max_text_length}")
        elif args.text_model == "albert-base":
            # Use ALBERT tokenizer for ALBERT model
            tokenizer = SimpleTokenizer(
                pretrained_model_name="albert-base-v2", max_length=args.max_text_length
            )
            print(f"Using ALBERT tokenizer with max length {args.max_text_length}")
        elif args.text_model == "bert-base":
            # Use BERT tokenizer for BERT model
            tokenizer = SimpleTokenizer(
                pretrained_model_name="bert-base-uncased",
                max_length=args.max_text_length,
            )
            print(f"Using BERT tokenizer with max length {args.max_text_length}")
        elif args.text_model == "roberta-base":
            # Use RoBERTa tokenizer for RoBERTa model
            tokenizer = SimpleTokenizer(
                pretrained_model_name="roberta-base", max_length=args.max_text_length
            )
            print(f"Using RoBERTa tokenizer with max length {args.max_text_length}")
        elif args.text_model == "distilbert-base":
            # Use DistilBERT tokenizer for DistilBERT model
            tokenizer = SimpleTokenizer(
                pretrained_model_name="distilbert-base-uncased",
                max_length=args.max_text_length,
            )
            print(f"Using DistilBERT tokenizer with max length {args.max_text_length}")
        elif args.text_model == "minilm-384":
            # Use MiniLM tokenizer
            tokenizer = SimpleTokenizer(
                pretrained_model_name="microsoft/MiniLM-L12-H384-uncased",
                max_length=args.max_text_length,
            )
            print(f"Using MiniLM tokenizer with max length {args.max_text_length}")
        elif args.text_model == "flaubert-small-cased":
            # Use FlauBERT tokenizer
            tokenizer = SimpleTokenizer(
                pretrained_model_name="flaubert-small-cased",
                max_length=args.max_text_length,
            )
            print(f"Using FlauBERT tokenizer with max length {args.max_text_length}")
        else:
            # Default to basic SimpleTokenizer for non-HuggingFace text models
            tokenizer = SimpleTokenizer(max_length=args.max_text_length)
            print(f"Using basic SimpleTokenizer with max length {args.max_text_length}")
    else:
        # For custom transformers, use the basic SimpleTokenizer
        tokenizer = SimpleTokenizer(max_length=args.max_text_length)
        print(f"Using basic SimpleTokenizer with max length {args.max_text_length}")

    if args.use_simple_model:
        logger.info("Using SimpleMultimodalModel for debugging")

        # Use your existing model but wrap it in SimpleMultimodalModel
        # First create the original model to get its components
        original_model = create_multimodal_model(args, device=device)

        # Extract the vision and text models
        vision_model = original_model.vision_model
        text_model = original_model.text_model

        # Create simple model using these components
        model = SimpleMultimodalModel(
            vision_model=vision_model,
            text_model=text_model,
            projection_dim=args.fusion_dim,
        )

        # Create supervised alignment loss with the user-provided temperature
        # Make sure to use a low temperature (0.07-0.1) for sharper contrast
        temperature = args.temperature if args.temperature is not None else 0.07
        print(f"Using temperature value: {temperature} (lower = sharper contrasts)")
        loss_fn = SupervisedAlignmentLoss(temperature=temperature)

        logger.info("Created simple model and supervised alignment loss")

        # Enable gradient checkpointing for memory efficiency if available
        if (
            hasattr(model.vision_model, "gradient_checkpointing_enable")
            and torch.cuda.is_available()
        ):
            model.vision_model.gradient_checkpointing_enable()
            logger.info("Enabled gradient checkpointing for vision model")

        if (
            hasattr(model.text_model, "gradient_checkpointing_enable")
            and torch.cuda.is_available()
        ):
            model.text_model.gradient_checkpointing_enable()
            logger.info("Enabled gradient checkpointing for text model")

        # Print detailed feature dimensions for debugging
        logger.info(f"Model projection dimensions: {args.fusion_dim}")

        # Initialize with stronger contrast between samples
        # This helps break symmetry and prevent feature collapse
        logger.info("Initializing models with stronger contrast:")
        for module in model.vision_proj.modules():
            if isinstance(module, nn.Linear):
                std = math.sqrt(2.0 / module.weight.size(1))
                nn.init.normal_(module.weight, mean=0.0, std=std * 1.2)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.01)

        for module in model.text_proj.modules():
            if isinstance(module, nn.Linear):
                std = math.sqrt(2.0 / module.weight.size(1))
                nn.init.normal_(module.weight, mean=0.0, std=std * 1.2)
                if module.bias is not None:
                    nn.init.constant_(module.bias, -0.01)

    else:
        # Original model creation
        model = create_multimodal_model(args, device=device)

    # if hasattr(args, "use_simple_model") and args.use_simple_model:
    #     logger.info("Using SimpleMultimodalModel architecture for debugging")

    #     # Create vision model
    #     logger.info(f"Creating vision model: {args.vision_model}")
    #     if args.use_pretrained:
    #         logger.info("Using pretrained weights for vision model")
    #     vision_model = create_vision_model(args)

    #     # Create text model
    #     logger.info(f"Creating text model: {args.text_model}")
    #     if args.use_pretrained_text:
    #         logger.info("Using pretrained weights for text model")
    #     text_model = create_text_model(args)

    #     # Create simple model
    #     model = SimpleMultimodalModel(
    #         vision_model=vision_model,
    #         text_model=text_model,
    #         projection_dim=args.fusion_dim,
    #     )

    #     logger.info(
    #         f"Created SimpleMultimodalModel with projection dim {args.fusion_dim}"
    #     )
    # else:
    #     # Original model creation code
    #     logger.info(f"Creating {args.fusion_type} fusion model")
    #     model = create_multimodal_model(args, device=device)

    # Create data loaders
    train_loader, val_loader, test_loader = create_data_loaders(
        args, image_preprocessor, tokenizer
    )

    # Check if we're using synthetic data
    is_synthetic = False
    if args.use_synthetic or args.dataset == "synthetic":
        is_synthetic = True
    else:
        # Get dataset info from the training dataset
        train_dataset_info = train_loader.dataset.get_split_proportions()
        dataset_name = train_dataset_info.get("dataset_name", "unknown")

        # Check if we're using real Flickr30k data
        if dataset_name == "flickr30k":
            # The data source is Flickr30k, check if we're using cached files from disk
            # or if we had to generate synthetic data
            cache_status = getattr(train_loader.dataset, "loaded_from_cache", True)
            is_synthetic = not cache_status  # If not loaded from cache, it's synthetic
        else:
            # For other dataset types or unknown sources, consider it synthetic
            is_synthetic = True

        # Add an extra warning if using a very small dataset
        train_dataset_size = train_dataset_info.get("total_samples", 0)
        if train_dataset_size < 100:
            print("\n" + "=" * 80)
            print("WARNING: Using a very small dataset (<100 samples).")
            print("Small datasets may lead to overfitting and unrealistic metrics.")
            print("Consider using more data for better evaluation.")
            print("=" * 80 + "\n")

    if is_synthetic:
        print("\n" + "=" * 80)
        print("WARNING: Using synthetic data for training and evaluation.")
        print("Results will not be representative of real-world performance.")
        print(
            "For real evaluation, please ensure you have proper access to the real dataset."
        )
        print("=" * 80 + "\n")

    # Get the dataset size for contrastive loss sampling strategy
    dataset_size = None
    try:
        train_dataset_info = train_loader.dataset.get_split_proportions()
        dataset_size = train_dataset_info.get("total_samples", None)
    except (AttributeError, TypeError):
        logger.warning(
            "Could not determine dataset size for contrastive sampling strategy"
        )
    # Create loss function with enhanced configuration
    if hasattr(args, "use_simple_model") and args.use_simple_model:
        # Create supervised alignment loss with higher temperature
        loss_fn = SupervisedAlignmentLoss(temperature=0.5)
        logger.info("Created SupervisedAlignmentLoss with temperature=0.5")
    else:
        # Original loss function creation
        # loss_fn = create_loss_function(args)
        loss_fn = create_loss_function(args, dataset_size, train_loader)
    # CRITICAL CHANGE: Initialize the model with pre-trained weights if possible
    # This helps avoid the "cold start" problem in contrastive learning
    try:
        if hasattr(model.vision_model, "initialize_from_pretrained"):
            logger.info("Initializing vision model from pretrained weights")
            success = model.vision_model.initialize_from_pretrained()
            if success:
                logger.info(
                    "Successfully initialized vision model from pretrained weights"
                )
            else:
                logger.warning(
                    "Failed to initialize vision model from pretrained weights"
                )

        if hasattr(model.text_model, "initialize_from_pretrained"):
            logger.info("Initializing text model from pretrained weights")
            success = model.text_model.initialize_from_pretrained()
            if success:
                logger.info(
                    "Successfully initialized text model from pretrained weights"
                )
            else:
                logger.warning(
                    "Failed to initialize text model from pretrained weights"
                )
    except Exception as e:
        logger.warning(f"Error initializing from pretrained: {str(e)}")

    # Create trainer with improved settings for contrastive learning
    # For models with feature collapse, we need special training settings
    if args.use_simple_model:
        # For the simple model, use a very specific learning strategy to prevent feature collapse
        trainer = MultimodalTrainer(
            model=model,
            train_dataloader=train_loader,
            val_dataloader=val_loader,
            test_dataloader=test_loader,
            loss_fn=loss_fn,
            num_epochs=args.num_epochs,
            learning_rate=args.learning_rate
            * 0.25,  # Reduced base LR, but projections have higher LR
            weight_decay=args.weight_decay
            * 2.0,  # Moderate weight decay for better generalization
            warmup_steps=int(
                max(500, args.warmup_steps * 1.5)
            ),  # Longer warmup for stability
            checkpoint_dir=os.path.join(args.output_dir, args.checkpoint_dir),
            log_dir=os.path.join(args.output_dir, args.log_dir),
            device=device,
            mixed_precision=args.use_mixed_precision,
            evaluation_steps=0,  # Only evaluate at the end of each epoch
            log_steps=25,  # Very frequent logging to closely track the collapse issue
            early_stopping_patience=10,  # More patience for slow learning process
            clip_grad_norm=0.5,  # Moderate gradient clipping to allow learning while preventing extremes
            accumulation_steps=2,  # Reduced accumulation steps for more frequent updates
            balance_modality_gradients=True,
        )
        logger.info(
            "Using special training settings for SimpleMultimodalModel to prevent feature collapse"
        )

        # Setup optimizer with layer-wise learning rates
        # Different learning rates for different parts of the model
        optimizer_grouped_parameters = [
            # Vision model gets very low learning rate
            {
                "params": [
                    p
                    for n, p in model.vision_model.named_parameters()
                    if p.requires_grad
                ],
                "lr": args.learning_rate * 0.005,  # Even lower LR for base vision model
                "weight_decay": args.weight_decay * 2.0,
                "name": "vision_model_params",
            },
            # Text model gets very low learning rate
            {
                "params": [
                    p for n, p in model.text_model.named_parameters() if p.requires_grad
                ],
                "lr": args.learning_rate * 0.005,  # Even lower LR for base text model
                "weight_decay": args.weight_decay * 2.0,
                "name": "text_model_params",
            },
            # Projection layers get higher learning rate
            {
                "params": [
                    p
                    for n, p in model.vision_proj.named_parameters()
                    if p.requires_grad
                ],
                "lr": args.learning_rate
                * 0.5,  # Much higher LR for projection layers to combat collapse
                "weight_decay": args.weight_decay * 2.0,
                "name": "vision_proj_params",
            },
            {
                "params": [
                    p for n, p in model.text_proj.named_parameters() if p.requires_grad
                ],
                "lr": args.learning_rate
                * 0.5,  # Much higher LR for projection layers to combat collapse
                "weight_decay": args.weight_decay * 2.0,
                "name": "text_proj_params",
            },
        ]

        # Create optimizer with these parameter groups
        optimizer = torch.optim.AdamW(optimizer_grouped_parameters)
        trainer.optimizer = optimizer
        logger.info("Created custom optimizer with layer-wise learning rates")

    else:
        # Regular settings for the full model
        trainer = MultimodalTrainer(
            model=model,
            train_dataloader=train_loader,
            val_dataloader=val_loader,
            test_dataloader=test_loader,
            loss_fn=loss_fn,
            num_epochs=args.num_epochs,
            learning_rate=args.learning_rate,  # Original learning rate
            weight_decay=args.weight_decay
            * 2.0,  # Double weight decay to prevent overfitting
            warmup_steps=args.warmup_steps,
            checkpoint_dir=os.path.join(args.output_dir, args.checkpoint_dir),
            log_dir=os.path.join(args.output_dir, args.log_dir),
            device=device,
            mixed_precision=args.use_mixed_precision,
            evaluation_steps=0,  # Only evaluate at the end of each epoch
            log_steps=100,  # Reduce logging frequency to avoid spam
            early_stopping_patience=5,
            clip_grad_norm=1.0,  # Enforce gradient clipping for stability
            accumulation_steps=2,  # Use gradient accumulation for more stable updates
            balance_modality_gradients=True,
        )

    # Train model
    print("Starting training...")
    print_model_summary(model, "TRAINING MULTIMODAL MODEL")

    # Use multi-stage training if enabled
    if args.use_multistage_training:
        # Determine which pretrained models we're using
        vision_status = "pretrained" if args.use_pretrained else "from scratch"
        text_status = "pretrained" if args.use_pretrained_text else "from scratch"
        freeze_status = "frozen" if args.freeze_base_models else "trainable"

        print(f"\nStarting multi-stage training with:")
        print(f"- Vision model: {args.vision_model} ({vision_status}, {freeze_status})")
        print(f"- Text model: {args.text_model} ({text_status}, {freeze_status})")

        # Add warning if using BERT with MPS
        is_bert = any(
            model_type in args.text_model.lower() for model_type in ["bert", "roberta"]
        )
        device_is_mps = next(model.parameters()).device.type == "mps"

        if is_bert and device_is_mps and args.use_pretrained_text:
            print("\n⚠️ INFO: Using BERT model on MPS device (Apple Silicon)")
            print(
                "The code now properly handles BERT models on MPS devices with a CPU fallback."
            )
            print("Processing may be slower but quality will not be affected.")

        # Determine if we're training from scratch
        # Either explicitly requested, or no pretrained weights are being used
        training_from_scratch = args.from_scratch or not (
            args.use_pretrained or args.use_pretrained_text
        )

        # If training from scratch, enable enhanced data augmentation
        if training_from_scratch:
            print("\nEnabling enhanced data augmentation for training from scratch")
            # This would normally modify dataloaders to include more augmentation strategies
            # For now, we're just acknowledging this would happen here
            # In a full implementation, we'd add more transforms, enable mixup, etc.
            train_dataloader = trainer.train_dataloader
            # TODO: Implement actual augmentation enhancement here

        # Print appropriate stage info based on training approach
        if training_from_scratch:
            print("\nTraining from scratch with specialized curriculum:")
            print("Stage 1a: Train early vision layers (edge/texture detection)")
            print("Stage 1b: Train early text layers (token-level understanding)")
            print("Stage 2a: Train mid-level vision layers (object parts)")
            print("Stage 2b: Train mid-level text layers (phrase-level understanding)")
            print("Stage 3: Train high-level representation in both modalities")
            print("Stage 4: Train cross-modal fusion")
            print("Stage 5: Fine-tune everything with hard negative mining")
        elif args.freeze_base_models:
            print("\nTraining stages (with pre-trained models, frozen):")
            print("Stage 1: Train projections only (vision and text models frozen)")
            print("Stage 2: Train fusion layers (vision and text models frozen)")
            print("Stage 3: Fine-tune everything together")
        else:
            print("\nTraining stages (with pre-trained models):")
            print(
                "Stage 1: Train all components with lower learning rate on pretrained models"
            )
            print("Stage 2: Increase focus on fusion layers")
            print("Stage 3: Fine-tune everything with hard negative mining")

        trainer.train_multistage()
    else:
        if args.freeze_base_models:
            print(
                "\nStarting standard training with FROZEN base models (only fusion layers will train)..."
            )
        else:
            print("\nStarting standard training with ALL parameters trainable...")
        trainer.train()

    # Print model summary after training
    print_model_summary(model, "MULTIMODAL MODEL AFTER TRAINING")

    # Run final evaluation
    print("Running final evaluation...")
    print_model_summary(model, "EVALUATION MODEL")
    test_metrics = trainer.evaluate(test_loader)

    # Add this code to visualize test samples
    print("Generating test sample visualizations...")

    # Run inference demo to get visualizations
    inference_metrics = run_inference_demo(
        model, image_preprocessor, tokenizer, device, args
    )

    # Convert PyTorch tensors to Python types for display and serialization
    python_test_metrics = convert_tensors_to_python_types(test_metrics)
    python_inference_metrics = convert_tensors_to_python_types(inference_metrics)

    # Create dictionaries with both metrics and model info
    complete_test_metrics = {
        "metrics": python_test_metrics,
        "model_info": {
            "total_parameters": count_parameters(model),
            "vision_model": args.vision_model,
            "text_model": args.text_model,
            "fusion_type": args.fusion_type,
            "fusion_dim": args.fusion_dim,
        },
    }

    complete_inference_metrics = {
        "metrics": python_inference_metrics,
        "model_info": {
            "total_parameters": count_parameters(model),
            "vision_model": args.vision_model,
            "text_model": args.text_model,
            "fusion_type": args.fusion_type,
            "fusion_dim": args.fusion_dim,
        },
    }

    if is_synthetic:
        print(f"Test metrics (SYNTHETIC DATA): {python_test_metrics}")
    else:
        print(f"Test metrics: {python_test_metrics}")

    # Save final test metrics
    import json

    with open(os.path.join(args.output_dir, "test_metrics.json"), "w") as f:
        json.dump(complete_test_metrics, f, indent=2)

    # Save demo results
    with open(os.path.join(args.output_dir, "demo_results.json"), "w") as f:
        json.dump(complete_inference_metrics, f, indent=2)

    if is_synthetic:
        print("Multimodal training demo completed with SYNTHETIC DATA!")
        print(
            "For real evaluation, please download and use the actual Flickr30k dataset."
        )
    else:
        print("Multimodal training demo completed!")


if __name__ == "__main__":
    main()
